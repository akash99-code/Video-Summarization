{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174a72d4-b312-4e88-806a-ed94f62e2945",
   "metadata": {},
   "source": [
    "## Transnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd768e2-462c-413b-a890-56a38a4c4898",
   "metadata": {},
   "source": [
    "**installations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c145a59-b22e-49b6-a5aa-9c21420d4d56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ffmpeg in c:\\users\\msc 2\\anaconda3\\lib\\site-packages (1.4)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\msc 2\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\msc 2\\anaconda3\\lib\\site-packages (9.2.0)\n",
      "Requirement already satisfied: future in c:\\users\\msc 2\\anaconda3\\lib\\site-packages (from ffmpeg-python) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg\n",
    "!pip install ffmpeg-python pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e396a89-2712-4665-ac74-a8834fb318cc",
   "metadata": {},
   "source": [
    "**packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f67310f-fd3f-4ed8-a0bc-88b06cb8b792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a806fe2-d062-4b70-86f6-81ed660737d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### )) Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2273688-fa20-4e32-bca8-1501f7276d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path='../../data'\n",
    "public_dataset_path=datasets_path+'/Public datasets'\n",
    "custom_data = datasets_path+'/Custom data'\n",
    "\n",
    "tvsum_data = public_dataset_path+'/ydata-tvsum50-v1_1'\n",
    "summe_data = public_dataset_path+'/SUMMe'\n",
    "\n",
    "result_path= 'transnet_segments/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542910e-bbc0-401e-aa88-aee2a4a74c3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### )) Transnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3156161-9fa1-4488-b922-5c59ec31a77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransNetV2:\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        model_dir = \"transnetv2-weights/\"\n",
    "        self._input_size = (27, 48, 3)\n",
    "        try:\n",
    "            self._model = tf.saved_model.load(model_dir)\n",
    "        except OSError as exc:\n",
    "            raise IOError(f\"[TransNetV2] It seems that files in {model_dir} are corrupted or missing. \"\n",
    "                          f\"Re-download them manually and retry. For more info, see: \"\n",
    "                          f\"https://github.com/soCzech/TransNetV2/issues/1#issuecomment-647357796\") from exc\n",
    "\n",
    "    def predict_raw(self, frames: np.ndarray):\n",
    "        assert len(frames.shape) == 5 and frames.shape[2:] == self._input_size, \\\n",
    "            \"[TransNetV2] Input shape must be [batch, frames, height, width, 3].\"\n",
    "        frames = tf.cast(frames, tf.float32)\n",
    "        logits, dict_ = self._model(frames)\n",
    "        single_frame_pred = tf.sigmoid(logits)\n",
    "        # all_frames_pred = tf.sigmoid(dict_[\"many_hot\"])\n",
    "        \n",
    "        return single_frame_pred\n",
    "    # , all_frames_pred\n",
    "\n",
    "    def predict_frames(self, frames: np.ndarray):\n",
    "        assert len(frames.shape) == 4 and frames.shape[1:] == self._input_size, \"[TransNetV2] Input shape must be [frames, height, width, 3].\"\n",
    "\n",
    "        def input_iterator():\n",
    "            # return windows of size 100 where the first/last 25 frames are from the previous/next batch\n",
    "            # the first and last window must be padded by copies of the first and last frame of the video\n",
    "            no_padded_frames_start = 25\n",
    "            no_padded_frames_end = 25 + 50 - (len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74\n",
    "\n",
    "            start_frame = np.expand_dims(frames[0], 0)\n",
    "            end_frame = np.expand_dims(frames[-1], 0)\n",
    "            padded_inputs = np.concatenate(\n",
    "                [start_frame] * no_padded_frames_start + [frames] + [end_frame] * no_padded_frames_end, 0\n",
    "            )\n",
    "\n",
    "            ptr = 0\n",
    "            while ptr + 100 <= len(padded_inputs):\n",
    "                out = padded_inputs[ptr:ptr + 100]\n",
    "                ptr += 50\n",
    "                yield out[np.newaxis]\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for inp in input_iterator():\n",
    "            # single_frame_pred, all_frames_pred = self.predict_raw(inp)\n",
    "            single_frame_pred = self.predict_raw(inp)\n",
    "            predictions.append((single_frame_pred.numpy()[0, 25:75, 0]))\n",
    "\n",
    "            print(\"\\r[TransNetV2] Processing video frames {}/{}\".format(\n",
    "                min(len(predictions) * 50, len(frames)), len(frames)\n",
    "            ), end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "        single_frame_pred = np.concatenate([single_ for single_ in predictions])\n",
    "        # all_frames_pred = np.concatenate([all_ for single_, all_ in predictions])\n",
    "\n",
    "        # return single_frame_pred[:len(frames)], all_frames_pred[:len(frames)]  # remove extra padded frames\n",
    "        return single_frame_pred[:len(frames)]\n",
    "\n",
    "    def predict_video(self, video_fn: str):\n",
    "\n",
    "        print(\"[TransNetV2] Extracting frames from {}\".format(video_fn))\n",
    "        video_stream, err = ffmpeg.input(video_fn).output(\n",
    "            \"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\", s=\"48x27\"\n",
    "        ).run(capture_stdout=True, capture_stderr=True)\n",
    "\n",
    "        video = np.frombuffer(video_stream, np.uint8).reshape([-1, 27, 48, 3])\n",
    "        return self.predict_frames(video)\n",
    "\n",
    "    @staticmethod\n",
    "    def predictions_to_scenes(predictions: np.ndarray, threshold: float = 0.5):\n",
    "        predictions = (predictions > threshold).astype(np.uint8)\n",
    "\n",
    "        scenes = []\n",
    "        t, t_prev, start = -1, 0, 0\n",
    "        for i, t in enumerate(predictions):\n",
    "            if t_prev == 1 and t == 0:\n",
    "                start = i\n",
    "            if t_prev == 0 and t == 1 and i != 0:\n",
    "                scenes.append([start, i])\n",
    "            t_prev = t\n",
    "        if t == 0:\n",
    "            scenes.append([start, i])\n",
    "\n",
    "        # just fix if all predictions are 1\n",
    "        if len(scenes) == 0:\n",
    "            return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n",
    "\n",
    "        return np.array(scenes, dtype=np.int32)\n",
    "\n",
    "\n",
    "def main(args, mappings=None):\n",
    "    model = TransNetV2()\n",
    "    video_folder = args['videoFolder_path']+'/video/'\n",
    "    files = os.listdir(video_folder)\n",
    "    c=0\n",
    "    \n",
    "    \n",
    "    with h5py.File(args['cpsH5_path'],'a') as d:\n",
    "        for file in files:\n",
    "\n",
    "            # video_frames, \n",
    "            # single_frame_predictions= model.predict_video(video_folder+file)\n",
    "            # scenes = model.predictions_to_scenes(single_frame_predictions)\n",
    "            \n",
    "            name = 'video_'+str(mappings.index[file.split['.'][0]]+1)\n",
    "            \n",
    "            if mappings!=None:\n",
    "                 name = 'video_'+str(mappings.index[file.split('.')[0]]+1)\n",
    "            else:\n",
    "                name = 'video_'+str(c)\n",
    "                \n",
    "            print(c+'. '+file+' as '+name+'; no.of shots='+len(scenes))\n",
    "            d.create_dataset(name+'/change_points', data=scenes )\n",
    "            d.create_dataset(name+'/video_name', data=file )\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcf448-9af7-4782-83d8-a2e403e15af6",
   "metadata": {},
   "source": [
    "#### )) TVSum segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d106f99b-72b5-40bb-97b5-8749ef56299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arguments\n",
    "args={\n",
    "    'videoFolder_path': tvsum_data,\n",
    "    'cpsH5_path': result_path+'/tvsumSegs.h5',    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b60f5ac4-8955-449e-a96e-4d723e762266",
   "metadata": {},
   "outputs": [],
   "source": [
    "## video name mappings\n",
    "tvsum_info=pd.read_csv(tvsum_data+'/data/ydata-tvsum50-info.tsv',sep='\\t')\n",
    "vnames = tvsum_info['video_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "522b299c-2a8f-4516-9c9d-bf85bd45b6d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## generate segments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args, mappings)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpsH5_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m d:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m         \u001b[38;5;66;03m# video_frames, \u001b[39;00m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;66;03m# single_frame_predictions= model.predict_video(video_folder+file)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;66;03m# scenes = model.predictions_to_scenes(single_frame_predictions)\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mappings\u001b[38;5;241m.\u001b[39mindex[\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mappings\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m              name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mappings\u001b[38;5;241m.\u001b[39mindex[file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "## generate segments\n",
    "main(args, vnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7c3f29a-760a-4c10-b583-107bd04dd933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yfuj', 'hyfj']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='yfuj.hyfj'\n",
    "x.split('.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

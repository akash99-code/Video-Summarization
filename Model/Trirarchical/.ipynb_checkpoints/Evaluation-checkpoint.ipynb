{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0ff0ab-2298-4881-b128-99122fa08825",
   "metadata": {},
   "source": [
    "## Evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "255d958a-67bf-47d6-b08a-ae5524834db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install import-ipynb\n",
    "## !pip3 install ortools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7a731b-4016-49b4-a2e0-f111ca4835f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Model import VASNet\n",
    "\n",
    "from ortools.algorithms import pywrapknapsack_solver\n",
    "\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606fa0f-fa13-4a77-83d2-e312c3023e2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### )) Util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2d66db-ecf8-4137-b7f7-159b79092bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knapsack_ortools(values, weights, items, capacity ):\n",
    "    scale = 1000\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    values = (values * scale).astype(np.int32)\n",
    "    weights = (weights).astype(np.int32)\n",
    "    capacity = capacity\n",
    "    osolver = pywrapknapsack_solver.KnapsackSolver(pywrapknapsack_solver.KnapsackSolver.KNAPSACK_DYNAMIC_PROGRAMMING_SOLVER,'test')\n",
    "    osolver.Init(values.tolist(), [weights.tolist()], [capacity])\n",
    "    computed_value = osolver.Solve()\n",
    "    packed_items = [x for x in range(0, len(weights))\n",
    "                    if osolver.BestSolutionContains(x)]\n",
    "\n",
    "    return packed_items\n",
    "\n",
    "\n",
    "\n",
    "def generate_summary(ypred, cps, n_frames, nfps, positions, proportion=0.15, method='knapsack'):\n",
    "    \"\"\"Generate keyshot-based video summary i.e. a binary vector.\n",
    "    Args:\n",
    "    ---------------------------------------------\n",
    "    - ypred: predicted importance scores.\n",
    "    - cps: change points, 2D matrix, each row contains a segment.\n",
    "    - n_frames: original number of frames.\n",
    "    - nfps: number of frames per segment.\n",
    "    - positions: positions of subsampled frames in the original video.\n",
    "    - proportion: length of video summary (compared to original video length).\n",
    "    - method: defines how shots are selected, ['knapsack', 'rank'].\n",
    "    \"\"\"\n",
    "    n_segs = cps.shape[0]\n",
    "    frame_scores = np.zeros((n_frames), dtype=np.float32)\n",
    "    if positions.dtype != int:\n",
    "        positions = positions.astype(np.int32)\n",
    "    if positions[-1] != n_frames:\n",
    "        positions = np.concatenate([positions, [n_frames]])\n",
    "    for i in range(len(positions) - 1):\n",
    "        pos_left, pos_right = positions[i], positions[i+1]\n",
    "        if i == len(ypred):\n",
    "            frame_scores[pos_left:pos_right] = 0\n",
    "        else:\n",
    "            frame_scores[pos_left:pos_right] = ypred[i]\n",
    "\n",
    "    seg_score = []\n",
    "    for seg_idx in range(n_segs):\n",
    "        start, end = int(cps[seg_idx,0]), int(cps[seg_idx,1]+1)\n",
    "        scores = frame_scores[start:end]\n",
    "        seg_score.append(float(scores.mean()))\n",
    "\n",
    "    limits = int(math.floor(n_frames * proportion))\n",
    "\n",
    "    if method == 'knapsack':\n",
    "        #picks = knapsack_dp(seg_score, nfps, n_segs, limits)\n",
    "        picks = knapsack_ortools(seg_score, nfps, n_segs, limits)\n",
    "    elif method == 'rank':\n",
    "        order = np.argsort(seg_score)[::-1].tolist()\n",
    "        picks = []\n",
    "        total_len = 0\n",
    "        for i in order:\n",
    "            if total_len + nfps[i] < limits:\n",
    "                picks.append(i)\n",
    "                total_len += nfps[i]\n",
    "    else:\n",
    "        raise KeyError(\"Unknown method {}\".format(method))\n",
    "\n",
    "    summary = np.zeros((1), dtype=np.float32) # this element should be deleted\n",
    "    for seg_idx in range(n_segs):\n",
    "        nf = nfps[seg_idx]\n",
    "        if seg_idx in picks:\n",
    "            tmp = np.ones((nf), dtype=np.float32)\n",
    "        else:\n",
    "            tmp = np.zeros((nf), dtype=np.float32)\n",
    "        summary = np.concatenate((summary, tmp))\n",
    "\n",
    "    summary = np.delete(summary, 0) # delete the first element\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_summary(machine_summary, user_summary, eval_metric='avg'):\n",
    "    \"\"\"Compare machine summary with user summary (keyshot-based).\n",
    "    Args:\n",
    "    --------------------------------\n",
    "    machine_summary and user_summary should be binary vectors of ndarray type.\n",
    "    eval_metric = {'avg', 'max'}\n",
    "    'avg' averages results of comparing multiple human summaries.\n",
    "    'max' takes the maximum (best) out of multiple comparisons.\n",
    "    \"\"\"\n",
    "    machine_summary = machine_summary.astype(np.float32)\n",
    "    user_summary = user_summary.astype(np.float32)\n",
    "    n_users,n_frames = user_summary.shape\n",
    "\n",
    "    # binarization\n",
    "    machine_summary[machine_summary > 0] = 1\n",
    "    user_summary[user_summary > 0] = 1\n",
    "\n",
    "    if len(machine_summary) > n_frames:\n",
    "        machine_summary = machine_summary[:n_frames]\n",
    "    elif len(machine_summary) < n_frames:\n",
    "        zero_padding = np.zeros((n_frames - len(machine_summary)))\n",
    "        machine_summary = np.concatenate([machine_summary, zero_padding])\n",
    "\n",
    "    f_scores = []\n",
    "    prec_arr = []\n",
    "    rec_arr = []\n",
    "\n",
    "    for user_idx in range(n_users):\n",
    "        gt_summary = user_summary[user_idx,:]\n",
    "        overlap_duration = (machine_summary * gt_summary).sum()\n",
    "        precision = overlap_duration / (machine_summary.sum() + 1e-8)\n",
    "        recall = overlap_duration / (gt_summary.sum() + 1e-8)\n",
    "        if precision == 0 and recall == 0:\n",
    "            f_score = 0.\n",
    "        else:\n",
    "            f_score = (2 * precision * recall) / (precision + recall)\n",
    "        f_scores.append(f_score)\n",
    "        prec_arr.append(precision)\n",
    "        rec_arr.append(recall)\n",
    "\n",
    "    if eval_metric == 'avg':\n",
    "        final_f_score = np.mean(f_scores)\n",
    "        final_prec = np.mean(prec_arr)\n",
    "        final_rec = np.mean(rec_arr)\n",
    "    elif eval_metric == 'max':\n",
    "        final_f_score = np.max(f_scores)\n",
    "        max_idx = np.argmax(f_scores)\n",
    "        final_prec = prec_arr[max_idx]\n",
    "        final_rec = rec_arr[max_idx]\n",
    "    \n",
    "    return final_f_score, final_prec, final_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7efc69-17a1-471c-b642-5e40cb18a2c6",
   "metadata": {},
   "source": [
    "#### )) Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea304505-3165-4de3-b935-190e89f4dfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, args):\n",
    "        self.use_cuda= args['use_cuda']\n",
    "        self.model_path= args['model_path']\n",
    "        self.data_path= args['featuresH5']\n",
    "        self.dataset_name= args['dataset_name']\n",
    "        self.Segs= args['SegH5'] if args['SegH5'] is not None else args['featuresH5']\n",
    "        self.results_path= args['results_path']\n",
    "        self.ifevaluate= args['ifgetScore']\n",
    " \n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = VASNet()\n",
    "        self.model.load_state_dict(torch.load(self.model_path, map_location=lambda storage, loc: storage))\n",
    "        self.model.eval()\n",
    "        return\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        summary = {}\n",
    "        att_vecs = {}\n",
    "        with torch.no_grad():\n",
    "            with h5py.File(self.data_path) as dataset:\n",
    "                keys=dataset.keys()\n",
    "                for i, key in enumerate(keys):\n",
    "                    seq = dataset[key]['features'][...]\n",
    "                    seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "\n",
    "                    if self.use_cuda:\n",
    "                        seq = seq.float().cuda()\n",
    "\n",
    "                    y, att_vec = self.model(seq, seq.shape[1])\n",
    "                    summary[key] = y[0].detach().cpu().numpy()\n",
    "                    att_vecs[key] = att_vec.detach().cpu().numpy()                   \n",
    "        results = self.eval_summary(summary, att_vecs=att_vecs, eval_metric=self.dataset_name)\n",
    "       \n",
    "        if results!=None:\n",
    "            f_score, video_scores = results\n",
    "            return f_score, video_scores\n",
    "        return\n",
    "            \n",
    "            \n",
    "    def eval_summary(self, machine_summary_activations, att_vecs, eval_metric='tvsum'):\n",
    "        \n",
    "        gen_ms=True\n",
    "        if Path(self.results_path).is_file():\n",
    "            with h5py.File(self.results_path, 'r') as h5_res:\n",
    "                key = list(h5_res.keys())\n",
    "                # print(key)\n",
    "                if 'machine_summary' in h5_res[key[0]].keys():\n",
    "                    gen_ms=False\n",
    "                    \n",
    "        # print(gen_ms)   \n",
    "        if gen_ms:\n",
    "            with h5py.File(self.Segs, 'r') as Segs, h5py.File(self.data_path, 'r') as d, h5py.File(self.results_path, 'a') as h5_res:  \n",
    "                akey = [k for k in Segs.keys()][0]\n",
    "                if 'change_points' not in Segs[akey]:\n",
    "                    print(\"ERROR: No change points in dataset/video \",key)\n",
    "                    return\n",
    "\n",
    "                akey = [k for k in d.keys()][0]\n",
    "                ifvidName = 'video_name' in d[akey]\n",
    "                \n",
    "\n",
    "                for key in  Segs.keys():  \n",
    "                    cps = Segs[key+'/change_points'][...]\n",
    "                    num_frames = d[key+'/n_frames'][()]\n",
    "                    nfps = d[key+'/n_frame_per_seg'][...].tolist()\n",
    "                    positions = d[key+'/picks'][...]\n",
    "\n",
    "                    probs = machine_summary_activations[key]\n",
    "                    machine_summary = generate_summary(probs, cps, num_frames, nfps, positions)\n",
    "                    h5_res.create_dataset(key + '/machine_summary', data=machine_summary)\n",
    "                    h5_res.create_dataset(key + '/score', data=probs)\n",
    "                    h5_res.create_dataset(key + '/picks', data=positions)\n",
    "                    if ifvidName:\n",
    "                        video_name = d[key+'/video_name'][...]\n",
    "                        h5_res.create_dataset(key + '/video_name', data=video_name)\n",
    "                     \n",
    "                    \n",
    "        with h5py.File(self.data_path, 'r') as d:\n",
    "            ifEvaluatable = 'user_summary' in d[list(d.keys())[0]].keys()\n",
    "        \n",
    "        if self.ifevaluate  and ifEvaluatable :\n",
    "            fms = []\n",
    "            video_scores = []\n",
    "            eval_metric = 'avg' if eval_metric == 'tvsum' else 'max'\n",
    "            with h5py.File(self.results_path, 'a') as h5_res, h5py.File(self.data_path, 'r') as d:\n",
    "                for key_idx, key in enumerate(d.keys()):\n",
    "                    user_summary = d[key+'/user_summary'][...]\n",
    "                    machine_summary = h5_res[key+'/machine_summary'][...]\n",
    "    \n",
    "                    fm, _, _ = evaluate_summary(machine_summary, user_summary, eval_metric)\n",
    "                    fms.append(fm)\n",
    "                    # Reporting & logging\n",
    "                    video_scores.append([key_idx + 1, key, \"{:.1%}\".format(fm)])\n",
    "                    gt = d[key+'/gt_score'][...]\n",
    "                    h5_res.create_dataset(key + '/gt_score', data=gt)\n",
    "                    h5_res.create_dataset(key + '/fm', data=fm)\n",
    "                    # h5_res[key]['gt_score'][...] =gt\n",
    "                    # h5_res[key]['fm'][...] = fm\n",
    "                    if att_vecs is not None:\n",
    "                        h5_res.create_dataset(key + '/att', data=att_vecs[key])\n",
    "                        # h5_res[key]['att'][...] = att_vecs[key]\n",
    "\n",
    "            mean_fm = np.mean(fms)\n",
    "            return fms, video_scores       \n",
    "        else:\n",
    "            return None\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413e05d-9e5f-48ab-88f0-e65d6f0fc5c3",
   "metadata": {},
   "source": [
    "#### )) Evaluate and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc8dcc-9481-41ce-91af-30cfb5a080c0",
   "metadata": {},
   "source": [
    "*1. Prebuilt dataset Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0069e371-3fc0-464b-963e-0d840ea43cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    'verbose':True,\n",
    "    'use_cuda':False,\n",
    "    'cuda_device':0,\n",
    "    'max_summary_length':0.15,\n",
    "    'featuresH5':'../../Preprocessing/extracted_features/Prebuilt/eccv16_dataset_tvsum_google_pool5.h5',\n",
    "    'SegH5':'../../Preprocessing/extracted_features/Prebuilt/eccv16_dataset_tvsum_google_pool5.h5',\n",
    "    'splits':None,\n",
    "    \"train\" : False,\n",
    "    \"model_path\" : 'models/tvsum_splits_4_0.5941821875878188.tar.pth', \n",
    "    \"dataset_name\": 'tvsum',\n",
    "    \"results_path\": 'results/tvsum_results.h5',\n",
    "    \"ifgetScore\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3d842afc-bffa-4700-ab40-39db8934df3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6789853686074021,\n",
       "  0.4277526982953381,\n",
       "  0.695260860539091,\n",
       "  0.6794149531433129,\n",
       "  0.5518940922056388,\n",
       "  0.5107115792509542,\n",
       "  0.6221873530487353,\n",
       "  0.6329145626346775,\n",
       "  0.6455173102543952,\n",
       "  0.7280821953243406,\n",
       "  0.6111263270584189,\n",
       "  0.5593201742233103,\n",
       "  0.6014167464413059,\n",
       "  0.49865804705805017,\n",
       "  0.5954089305074854,\n",
       "  0.5175790437998536,\n",
       "  0.6399780086242396,\n",
       "  0.5151939521970472,\n",
       "  0.5063074849594303,\n",
       "  0.5044567870988157,\n",
       "  0.48940607565959127,\n",
       "  0.5334207279692167,\n",
       "  0.6229330138439523,\n",
       "  0.5103378704035068,\n",
       "  0.7159873790728457,\n",
       "  0.6107339149256661,\n",
       "  0.654796106529214,\n",
       "  0.5515137205012631,\n",
       "  0.6146226381940736,\n",
       "  0.6266090275661015,\n",
       "  0.4238929462480964,\n",
       "  0.5195210940099223,\n",
       "  0.5925816977429421,\n",
       "  0.6188636462143663,\n",
       "  0.5806615000906904,\n",
       "  0.8225339056088858,\n",
       "  0.6705800864720498,\n",
       "  0.7104627097100793,\n",
       "  0.5846208536478136,\n",
       "  0.7269130584676178,\n",
       "  0.5672652018999378,\n",
       "  0.5505985226891198,\n",
       "  0.48877655816772875,\n",
       "  0.588277124352954,\n",
       "  0.7169275368382654,\n",
       "  0.5867166915561477,\n",
       "  0.5910150069625567,\n",
       "  0.56964215651383,\n",
       "  0.5058937732976425,\n",
       "  0.5233478743687284],\n",
       " [[1, 'video_1', '67.9%'],\n",
       "  [2, 'video_10', '42.8%'],\n",
       "  [3, 'video_11', '69.5%'],\n",
       "  [4, 'video_12', '67.9%'],\n",
       "  [5, 'video_13', '55.2%'],\n",
       "  [6, 'video_14', '51.1%'],\n",
       "  [7, 'video_15', '62.2%'],\n",
       "  [8, 'video_16', '63.3%'],\n",
       "  [9, 'video_17', '64.6%'],\n",
       "  [10, 'video_18', '72.8%'],\n",
       "  [11, 'video_19', '61.1%'],\n",
       "  [12, 'video_2', '55.9%'],\n",
       "  [13, 'video_20', '60.1%'],\n",
       "  [14, 'video_21', '49.9%'],\n",
       "  [15, 'video_22', '59.5%'],\n",
       "  [16, 'video_23', '51.8%'],\n",
       "  [17, 'video_24', '64.0%'],\n",
       "  [18, 'video_25', '51.5%'],\n",
       "  [19, 'video_26', '50.6%'],\n",
       "  [20, 'video_27', '50.4%'],\n",
       "  [21, 'video_28', '48.9%'],\n",
       "  [22, 'video_29', '53.3%'],\n",
       "  [23, 'video_3', '62.3%'],\n",
       "  [24, 'video_30', '51.0%'],\n",
       "  [25, 'video_31', '71.6%'],\n",
       "  [26, 'video_32', '61.1%'],\n",
       "  [27, 'video_33', '65.5%'],\n",
       "  [28, 'video_34', '55.2%'],\n",
       "  [29, 'video_35', '61.5%'],\n",
       "  [30, 'video_36', '62.7%'],\n",
       "  [31, 'video_37', '42.4%'],\n",
       "  [32, 'video_38', '52.0%'],\n",
       "  [33, 'video_39', '59.3%'],\n",
       "  [34, 'video_4', '61.9%'],\n",
       "  [35, 'video_40', '58.1%'],\n",
       "  [36, 'video_41', '82.3%'],\n",
       "  [37, 'video_42', '67.1%'],\n",
       "  [38, 'video_43', '71.0%'],\n",
       "  [39, 'video_44', '58.5%'],\n",
       "  [40, 'video_45', '72.7%'],\n",
       "  [41, 'video_46', '56.7%'],\n",
       "  [42, 'video_47', '55.1%'],\n",
       "  [43, 'video_48', '48.9%'],\n",
       "  [44, 'video_49', '58.8%'],\n",
       "  [45, 'video_5', '71.7%'],\n",
       "  [46, 'video_50', '58.7%'],\n",
       "  [47, 'video_6', '59.1%'],\n",
       "  [48, 'video_7', '57.0%'],\n",
       "  [49, 'video_8', '50.6%'],\n",
       "  [50, 'video_9', '52.3%']])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluator = Evaluation(args)\n",
    "# evaluator.init_model()\n",
    "# evaluator.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cafdf-412f-4f92-9d46-f4a84a5f1b6f",
   "metadata": {},
   "source": [
    "*On Normal features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e1be9f-d076-4551-ac54-0a2f8eb696a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    'verbose':True,\n",
    "    'use_cuda':False,\n",
    "    'cuda_device':0,\n",
    "    'max_summary_length':0.15,\n",
    "    'featuresH5':'../../Preprocessing/extracted_features/normal/TVSum.h5',\n",
    "    'SegH5':'../../Preprocessing/extracted_features/normal/TVSum.h5',\n",
    "    'splits':None,\n",
    "    \"train\" : False,\n",
    "    \"model_path\" : 'data/models/tvsum_splits_4_0.5941821875878188.tar.pth', \n",
    "    \"dataset_name\": 'tvsum',\n",
    "    \"results_path\": 'results/tvsum_results_normal.h5',\n",
    "    \"ifgetScore\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa1023f1-fddc-433e-9754-2e0b0f7d3f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6533333333291853,\n",
       "  0.5656130544440531,\n",
       "  0.608510638289241,\n",
       "  0.6396966368215052,\n",
       "  0.549999999989524,\n",
       "  0.6427083333244068,\n",
       "  0.7069767441750855,\n",
       "  0.57578947368017,\n",
       "  0.4724137930980182,\n",
       "  0.6651265423074588,\n",
       "  0.5831171563584225,\n",
       "  0.4380434782545211,\n",
       "  0.645141356530478,\n",
       "  0.6036082474206061,\n",
       "  0.6714285714205781,\n",
       "  0.6430158373865467,\n",
       "  0.5604651162703804,\n",
       "  0.6271340515837813,\n",
       "  0.5939393939273951,\n",
       "  0.5527522935746011,\n",
       "  0.6273406324641854,\n",
       "  0.6250960431411086,\n",
       "  0.5710714285687091,\n",
       "  0.5037499999916042,\n",
       "  0.48604809953119676,\n",
       "  0.6065789473577793,\n",
       "  0.5402255639070664,\n",
       "  0.505405405396299,\n",
       "  0.44545454544779617,\n",
       "  0.7297468354368799,\n",
       "  0.6074999999898749,\n",
       "  0.6722629793713344,\n",
       "  0.6768292682816776,\n",
       "  0.5635007823829392,\n",
       "  0.6259433695018601,\n",
       "  0.8009313482768112,\n",
       "  0.6644254630042482,\n",
       "  0.6256800400839204,\n",
       "  0.6267938008454603,\n",
       "  0.64799999998272,\n",
       "  0.6798646593774486,\n",
       "  0.5574468085027313,\n",
       "  0.5156628748989096,\n",
       "  0.648284986929099,\n",
       "  0.6821956779459928,\n",
       "  0.6565648597988768,\n",
       "  0.6302083333289569,\n",
       "  0.6840909090805439,\n",
       "  0.6566326530567578,\n",
       "  0.617390504226693],\n",
       " [[1, 'video_1', '65.3%'],\n",
       "  [2, 'video_10', '56.6%'],\n",
       "  [3, 'video_11', '60.9%'],\n",
       "  [4, 'video_12', '64.0%'],\n",
       "  [5, 'video_13', '55.0%'],\n",
       "  [6, 'video_14', '64.3%'],\n",
       "  [7, 'video_15', '70.7%'],\n",
       "  [8, 'video_16', '57.6%'],\n",
       "  [9, 'video_17', '47.2%'],\n",
       "  [10, 'video_18', '66.5%'],\n",
       "  [11, 'video_19', '58.3%'],\n",
       "  [12, 'video_2', '43.8%'],\n",
       "  [13, 'video_20', '64.5%'],\n",
       "  [14, 'video_21', '60.4%'],\n",
       "  [15, 'video_22', '67.1%'],\n",
       "  [16, 'video_23', '64.3%'],\n",
       "  [17, 'video_24', '56.0%'],\n",
       "  [18, 'video_25', '62.7%'],\n",
       "  [19, 'video_26', '59.4%'],\n",
       "  [20, 'video_27', '55.3%'],\n",
       "  [21, 'video_28', '62.7%'],\n",
       "  [22, 'video_29', '62.5%'],\n",
       "  [23, 'video_3', '57.1%'],\n",
       "  [24, 'video_30', '50.4%'],\n",
       "  [25, 'video_31', '48.6%'],\n",
       "  [26, 'video_32', '60.7%'],\n",
       "  [27, 'video_33', '54.0%'],\n",
       "  [28, 'video_34', '50.5%'],\n",
       "  [29, 'video_35', '44.5%'],\n",
       "  [30, 'video_36', '73.0%'],\n",
       "  [31, 'video_37', '60.7%'],\n",
       "  [32, 'video_38', '67.2%'],\n",
       "  [33, 'video_39', '67.7%'],\n",
       "  [34, 'video_4', '56.4%'],\n",
       "  [35, 'video_40', '62.6%'],\n",
       "  [36, 'video_41', '80.1%'],\n",
       "  [37, 'video_42', '66.4%'],\n",
       "  [38, 'video_43', '62.6%'],\n",
       "  [39, 'video_44', '62.7%'],\n",
       "  [40, 'video_45', '64.8%'],\n",
       "  [41, 'video_46', '68.0%'],\n",
       "  [42, 'video_47', '55.7%'],\n",
       "  [43, 'video_48', '51.6%'],\n",
       "  [44, 'video_49', '64.8%'],\n",
       "  [45, 'video_5', '68.2%'],\n",
       "  [46, 'video_50', '65.7%'],\n",
       "  [47, 'video_6', '63.0%'],\n",
       "  [48, 'video_7', '68.4%'],\n",
       "  [49, 'video_8', '65.7%'],\n",
       "  [50, 'video_9', '61.7%']])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluator = Evaluation(args)\n",
    "# evaluator.init_model()\n",
    "# evaluator.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0011613-288a-4ce8-bf47-1d11cfe90c5d",
   "metadata": {},
   "source": [
    "*On Test features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5ef0c2-f2d8-47fd-968d-4307c72265cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'verbose':True,\n",
    "    'use_cuda':False,\n",
    "    'cuda_device':0,\n",
    "    'max_summary_length':0.15,\n",
    "    'featuresH5':'../../Preprocessing/extracted_features/normal/Test1.h5',\n",
    "    'SegH5':'../../Preprocessing/extracted_features/normal/Test1.h5',\n",
    "    'splits':None,\n",
    "    \"train\" : False,\n",
    "    \"model_path\" : 'models/tvsum_splits_4_0.5941821875878188.tar.pth', \n",
    "    \"dataset_name\": 'tvsum',\n",
    "    \"results_path\": 'results/test_results_normal.h5',\n",
    "    \"ifgetScore\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f4f202-fe5d-423d-bb10-df21b9bce277",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation(args)\n",
    "evaluator.init_model()\n",
    "evaluator.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

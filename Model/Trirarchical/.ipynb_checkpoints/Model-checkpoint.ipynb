{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cefdcb-ac5d-45f2-9963-2487f18caea4",
   "metadata": {},
   "source": [
    "## Trirarchical Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dee9b-147b-486a-8a31-dac71c59db83",
   "metadata": {},
   "source": [
    "**packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6911d90f-bdb7-482f-8e85-0dc2b420ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd12d73e-b6d6-4269-9f92-09941db510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_probs(shot_probs, cps, n_frames, device):\n",
    "    if len(shot_probs) != len(cps):\n",
    "        print('no. of shots does not match:', len(shot_probs),len(cps))\n",
    "        return\n",
    "    frame_probs = torch.zeros(n_frames, dtype=torch.float32, device = device)\n",
    "    n_segs = cps.shape[0]\n",
    "    for seg_idx in range(n_segs):\n",
    "        first, last = cps[seg_idx]\n",
    "        first, last =  int(first.item()), int(last.item())\n",
    "        frame_probs[first:last + 1] = shot_probs[seg_idx]\n",
    "        \n",
    "    return frame_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dea82-e275-4a1d-9387-4f5a6f805e16",
   "metadata": {},
   "source": [
    "#### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b1e3f1-7a23-4c7a-a5ec-f2add7a4514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "#         self.permitted_encodings = [\"absolute\", \"relative\"]\n",
    "#         if pos_enc is not None:\n",
    "#             pos_enc = pos_enc.lower()\n",
    "#             assert pos_enc in self.permitted_encodings, f\"Supported encodings: {*self.permitted_encodings,}\"\n",
    "\n",
    "        self.heads = heads\n",
    "        self.pos_enc = pos_enc\n",
    "        self.freq = freq\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.Wq, self.Wk, self.Wv = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.heads):\n",
    "            self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            \n",
    "        self.W0 = nn.Linear(in_features=input_size, out_features=input_size, bias=False)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.W2 = nn.Linear(in_features=input_size, out_features=input_size, bias=False)\n",
    "        self.LN = nn.LayerNorm(normalized_shape=self.W2.out_features, eps=1e-6)\n",
    "    \n",
    "    \n",
    "########################################## Positional Encoding Code ###############################\n",
    "#     def getAbsolutePosition(self, T):\n",
    "#         \"\"\"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\n",
    "#         Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\n",
    "#         :param int T: Number of frames contained in Q, K and V\n",
    "#         :return: Tensor with shape [T, T]\n",
    "#         \"\"\"\n",
    "#         freq = self.freq\n",
    "#         d = self.input_size\n",
    "\n",
    "#         pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "#         i = torch.tensor([k for k in range(T//2)], device=self.out.weight.device)\n",
    "\n",
    "#         # Reshape tensors each pos_k for each i indices\n",
    "#         pos = pos.reshape(pos.shape[0], 1)\n",
    "#         pos = pos.repeat_interleave(i.shape[0], dim=1)\n",
    "#         i = i.repeat(pos.shape[0], 1)\n",
    "\n",
    "#         AP = torch.zeros(T, T, device=self.out.weight.device)\n",
    "#         AP[pos, 2*i] = torch.sin(pos / freq ** ((2 * i) / d))\n",
    "#         AP[pos, 2*i+1] = torch.cos(pos / freq ** ((2 * i) / d))\n",
    "#         return AP\n",
    "\n",
    "#     def getRelativePosition(self, T):\n",
    "#         \"\"\"Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\n",
    "#         r_pos calculations as here: https://theaisummer.com/positional-embeddings/\n",
    "#         :param int T: Number of frames contained in Q, K and V\n",
    "#         :return: Tensor with shape [T, T]\n",
    "#         \"\"\"\n",
    "#         freq = self.freq\n",
    "#         d = 2 * T\n",
    "#         min_rpos = -(T - 1)\n",
    "\n",
    "#         i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "#         j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "\n",
    "#         # Reshape tensors each i for each j indices\n",
    "#         i = i.reshape(i.shape[0], 1)\n",
    "#         i = i.repeat_interleave(i.shape[0], dim=1)\n",
    "#         j = j.repeat(i.shape[0], 1)\n",
    "\n",
    "#         # Calculate the relative positions\n",
    "#         r_pos = j - i - min_rpos\n",
    "\n",
    "#         RP = torch.zeros(T, T, device=self.out.weight.device)\n",
    "#         idx = torch.tensor([k for k in range(T//2)], device=self.out.weight.device)\n",
    "#         RP[:, 2*idx] = torch.sin(r_pos[:, 2*idx] / freq ** ((i[:, 2*idx] + j[:, 2*idx]) / d))\n",
    "#         RP[:, 2*idx+1] = torch.cos(r_pos[:, 2*idx+1] / freq ** ((i[:, 2*idx+1] + j[:, 2*idx+1]) / d))\n",
    "#         return RP\n",
    "###################################################################################################\n",
    "\n",
    "    def forward(self, x, batch_mask=None):\n",
    "\n",
    "        embs = []\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            K = self.Wk[head](x)\n",
    "            Q = self.Wq[head](x)\n",
    "            V = self.Wv[head](x)\n",
    "            \n",
    "\n",
    "            # Q *= 0.06                            # scale factor\n",
    "            \n",
    "            energies = torch.matmul(Q, K.transpose(1, 2))/np.sqrt(self.output_size//self.heads)\n",
    "            \n",
    "            ######### Positional Encoding skipped ##############\n",
    "            # if self.pos_enc is not None:\n",
    "            #     if self.pos_enc == \"absolute\":\n",
    "            #         AP = self.getAbsolutePosition(T=energies.shape[0])\n",
    "            #         energies = energies + AP\n",
    "            #     elif self.pos_enc == \"relative\":\n",
    "            #         RP = self.getRelativePosition(T=energies.shape[0])\n",
    "            #         energies = energies + RP\n",
    "            ###########################################\n",
    "            \n",
    "            \n",
    "            if batch_mask is not None:\n",
    "                _att = self.softmax(energies + batch_mask*-1e10)\n",
    "            else:\n",
    "                _att = self.softmax(energies)\n",
    "            att_weights = self.drop(_att)\n",
    "            emb = torch.matmul(att_weights, V)\n",
    "            \n",
    "            # Save the current head output\n",
    "            embs.append(emb)\n",
    "            \n",
    "        embs = self.W0(torch.cat(embs, dim=-1))\n",
    "        embs = self.relu(embs)\n",
    "        embs = self.W2(embs)\n",
    "        embs = self.LN(embs)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a48092-969c-4f86-a845-d7b021bce9c8",
   "metadata": {},
   "source": [
    "### Frame and Audio Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242a7f9a-049c-4f9c-9f0a-c15a657e3be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class FrameAndAudioAttention(nn.Module):\n",
    "    def __init__(self, vid_input_size=1024, aud_input_size=128, freq=10000, heads=1, pos_enc=None):\n",
    "        \n",
    "        super(FrameAndAudioAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.pos_enc = pos_enc\n",
    "        self.freq = freq\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        ##### Attention for Frames\n",
    "        self.vid_input_size = vid_input_size\n",
    "        self.vid_output_size = vid_input_size \n",
    "        self.Wq_v, self.Wk_v, self.Wv_v = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(self.heads):\n",
    "            self.Wk_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wq_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wv_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            \n",
    "        self.v_W0 = nn.Linear(in_features=vid_input_size, out_features=vid_input_size, bias=False)\n",
    "        self.v_drop = nn.Dropout(p=0.5)\n",
    "        self.v_W2 = nn.Linear(in_features=vid_input_size, out_features=vid_input_size, bias=False)\n",
    "        self.v_LN = nn.LayerNorm(normalized_shape=self.v_W2.out_features, eps=1e-6)\n",
    "        \n",
    "        \n",
    "        ##### Attention for Audios\n",
    "        self.aud_input_size = aud_input_size\n",
    "        self.aud_output_size = aud_input_size\n",
    "        self.Wk_a, self.Wv_a = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(self.heads):\n",
    "            self.Wk_a.append(nn.Linear(in_features=aud_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wv_a.append(nn.Linear(in_features=aud_input_size, out_features=aud_input_size//heads, bias=False))\n",
    "            \n",
    "        self.a_W0 = nn.Linear(in_features=aud_input_size, out_features=aud_input_size, bias=False)\n",
    "        self.a_drop = nn.Dropout(p=0.5)\n",
    "        self.a_W2 = nn.Linear(in_features=aud_input_size, out_features=aud_input_size, bias=False)\n",
    "        self.a_LN = nn.LayerNorm(normalized_shape=self.a_W2.out_features, eps=1e-6)\n",
    "\n",
    "    def forward(self, vid, aud, batch_mask):\n",
    "\n",
    "        vid_embs = []\n",
    "        aud_embs = []\n",
    "\n",
    "        for head in range(self.heads):\n",
    "            K_v = self.Wk_v[head](vid)\n",
    "            Q_v = self.Wq_v[head](vid)\n",
    "            V_v = self.Wv_v[head](vid)\n",
    "\n",
    "            K_a = self.Wk_a[head](aud)\n",
    "            V_a = self.Wv_a[head](aud)\n",
    "\n",
    "\n",
    "            energies_vid = torch.matmul(Q_v, K_v.transpose(1, 2))/np.sqrt(self.vid_output_size//self.heads)\n",
    "            energies_aud = torch.matmul(Q_v, K_a.transpose(1, 2))/np.sqrt(self.aud_output_size//self.heads)\n",
    "\n",
    "                  \n",
    "            vid_att = self.softmax(energies_vid+batch_mask*-1e10)\n",
    "            vid_att_weights = self.v_drop(vid_att)\n",
    "            vid_emb = torch.matmul(vid_att_weights, V_v)\n",
    "\n",
    "            aud_att = self.softmax(energies_aud+batch_mask*-1e10)\n",
    "            aud_att_weights = self.a_drop(aud_att)\n",
    "            aud_emb = torch.matmul(aud_att_weights, V_a)\n",
    "\n",
    "\n",
    "            # Save the current head output\n",
    "            vid_embs.append(vid_emb)\n",
    "            aud_embs.append(aud_emb)\n",
    "\n",
    "\n",
    "        vid_embs = self.v_W0(torch.cat(vid_embs, dim=-1))\n",
    "\n",
    "        vid_embs = self.relu(vid_embs)\n",
    "        vid_embs =self.v_W2(vid_embs)\n",
    "        vid_embs =self.v_LN(vid_embs)\n",
    "\n",
    "        aud_embs = self.a_W0(torch.cat(aud_embs, dim=-1))\n",
    "        aud_embs = self.relu(aud_embs)\n",
    "        aud_embs =self.a_W2(aud_embs)\n",
    "        aud_embs =self.a_LN(aud_embs)\n",
    "\n",
    "\n",
    "        return vid_embs, aud_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9158def-024b-4932-957b-91f73287004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAwareFusion(nn.Module):\n",
    "    def __init__(self, input_size, heads=1, pos_enc=None):\n",
    "        \n",
    "        super(AttentionAwareFusion, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features=input_size, out_features=input_size, bias=True)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.W2 = nn.Linear(in_features=input_size, out_features=input_size, bias=True)\n",
    "        self.sm = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.gelu(self.W1(x))\n",
    "        A = self.W2(Q)\n",
    "        att = self.sm(A.permute(0,2,1))\n",
    "        x = att.permute(0,2,1)*x\n",
    "        return x.sum(dim=1)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb8f0a-0a31-4c4d-ab8a-f3693c6bb646",
   "metadata": {},
   "source": [
    "### Trirarchical Multimodal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3aa343fa-96a3-485f-a5d7-a7d7a38aed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trirar(nn.Module):\n",
    "    def __init__(self, vid_input_size=1024, aud_input_size=128, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n",
    "\n",
    "        super(Trirar, self).__init__()\n",
    "        \n",
    "        self.FrameAndAudioAttention = FrameAndAudioAttention(vid_input_size, aud_input_size, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "        self.Shot_attention = MultiheadAttention(input_size=1024+128, output_size=1024+128, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "        self.Scorer = nn.Linear(in_features=1024+128, out_features=1)\n",
    "        self.probs = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.ShotsInteraction = MultiheadAttention(input_size=1024+128, output_size=1024+128, freq=freq, pos_enc=pos_enc, heads=4)  ##Can incoporate text feats\n",
    "        self.Scene_attention = MultiheadAttention(input_size=1024+128, output_size=1024+128, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "        self.SceneScorer = nn.Linear(in_features=1024+128, out_features=1)\n",
    "\n",
    "       \n",
    "\n",
    "        ################# Global Attention ############\n",
    "        # self.glob_attention = SelfAttention(input_size=input_size, output_size=output_size,\n",
    "        #                                freq=freq, pos_enc=pos_enc, heads=heads)\n",
    "\n",
    "        \n",
    "    def forward(self, x, a, shot_boundaries, scene_boundaries, batch_size=5):\n",
    "        shot_batch=[]\n",
    "        aud_batch=[]\n",
    "        batch_mask=[]\n",
    "        bs_counter=0\n",
    "        # weighted_value, attn_weights = self.glob_attention(x)  # global attention\n",
    "        \n",
    "        Shot_embeddings_list=[]\n",
    "        boundaries_num = len(shot_boundaries)\n",
    "        start=0\n",
    "        for shotnum, (start, end) in enumerate(shot_boundaries):\n",
    "            # end=math.ceil(end/15)\n",
    "            \n",
    "            shot= x[0][start:end+1]\n",
    "            audio= a[0][start:end+1]\n",
    "            # shot= x[0][start:end]\n",
    "            # print('shot', shot, start,end)\n",
    "            # audio= a[0][start:end]\n",
    "            batch_mask.append(torch.zeros(shot.shape[0], shot.shape[0]))\n",
    "            # start = end+1\n",
    "            \n",
    "            shot_batch.append(shot)\n",
    "            aud_batch.append(audio)\n",
    "            \n",
    "            bs_counter+=1\n",
    "            if bs_counter != batch_size and shotnum!=boundaries_num-1:\n",
    "                continue\n",
    "                \n",
    "            bs_counter=0\n",
    "            \n",
    "            shot_batch = pad_sequence(shot_batch, batch_first=True)\n",
    "            aud_batch = pad_sequence(aud_batch, batch_first=True)\n",
    "            \n",
    "            pad_length = shot_batch[0].shape[0]\n",
    "            batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "            # print('####', 'shot_batch', shot_batch,'aud_batch',  aud_batch,'batch_mask',  batch_mask.to(next(self.parameters()).device), '####')\n",
    "                  \n",
    "            sv, sa = self.FrameAndAudioAttention(shot_batch, aud_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "            # print('SV, SA', sv, sa)\n",
    "            shot_emb = torch.mean(sv,1)\n",
    "            audio_emb = torch.mean(sa,1)\n",
    "            Shot_embeddings_list.append(torch.hstack((shot_emb,audio_emb)))\n",
    "            del(shot_batch)\n",
    "            del(aud_batch)\n",
    "            shot_batch=[]\n",
    "            aud_batch=[]\n",
    "            del(batch_mask)\n",
    "            batch_mask=[]\n",
    "            \n",
    "        # print(len(Shot_embeddings_list))\n",
    "        Shot_embeddings = torch.cat(Shot_embeddings_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "        E2 = self.Shot_attention(Shot_embeddings)\n",
    "        \n",
    "        \n",
    "        P = self.Scorer(E2.squeeze(0))         ##  ???\n",
    "        \n",
    "        Scene_embeddings_list = []\n",
    "        batch_mask=[]\n",
    "        bs_counter=0\n",
    "        scene_batch = []\n",
    "        boundaries_num = len(scene_boundaries)\n",
    "        for scnnum, (start, end) in enumerate(scene_boundaries):\n",
    "            \n",
    "            scene=Shot_embeddings[0][start:end+1]\n",
    "            # print(scene.shape)\n",
    "            if end+1-start >5:\n",
    "                \n",
    "                if len(scene_batch) !=0:\n",
    "                    \n",
    "                    scene_batch = pad_sequence(scene_batch, batch_first=True)\n",
    "                    pad_length = scene_batch[0].shape[0]\n",
    "                    batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "                    \n",
    "                    sc = self.ShotsInteraction(scene_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "                    scene_emb = torch.mean(sc, 1)\n",
    "                    Scene_embeddings_list.append(scene_emb)\n",
    "                    \n",
    "                    del(batch_mask)\n",
    "                    del(scene_batch)\n",
    "                    batch_mask=[]\n",
    "                    bs_counter=0\n",
    "                    scene_batch = []\n",
    "                    \n",
    "                sc = self.ShotsInteraction(scene.unsqueeze(0))\n",
    "                scene_emb = torch.mean(sc, 1)\n",
    "                Scene_embeddings_list.append(scene_emb)\n",
    "                \n",
    "            else:\n",
    "                scene_batch.append(scene)\n",
    "                batch_mask.append(torch.zeros(end+1-start, end+1-start))\n",
    "                bs_counter+=1\n",
    "                if bs_counter != batch_size and scnnum!=boundaries_num-1:\n",
    "                    continue\n",
    "\n",
    "                scene_batch = pad_sequence(scene_batch, batch_first=True)\n",
    "                pad_length = scene_batch[0].shape[0]\n",
    "                batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "                \n",
    "                sc = self.ShotsInteraction(scene_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "                scene_emb = torch.mean(sc, 1)\n",
    "                Scene_embeddings_list.append(scene_emb)\n",
    "                \n",
    "                del(batch_mask)\n",
    "                del(scene_batch)\n",
    "                batch_mask=[]\n",
    "                bs_counter=0\n",
    "                scene_batch = []\n",
    "            \n",
    "        \n",
    "        Scene_embeddings = torch.cat(Scene_embeddings_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "        E3 = self.Scene_attention(Scene_embeddings)\n",
    "        S = self.SceneScorer(E3.squeeze(0))\n",
    "        \n",
    "        for scn, (start, end) in enumerate(scene_boundaries):\n",
    "            P[start:end+1]+= S[scn]\n",
    "        \n",
    "        P = self.probs(P.squeeze(0))\n",
    "        \n",
    "       \n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e083c24-3faf-4cc8-8b98-4c0f3f7c65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a6319-430f-4976-a516-a66e0fdac374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288abd15-1235-49fb-b27a-afd1b2be9f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cb4bf-7803-4568-98db-6cd76c30316a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e70a3-874b-428e-ad8c-1a569464c14c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb2f02-6f9d-4a7a-a8c0-ed2a604ce9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22a1db4-1b61-4858-a54a-e285fe039b82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### )) Trirar 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee4fef8c-5104-48b7-90e4-43b019ec2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trirar2(nn.Module):\n",
    "#     def __init__(self, vid_input_size=1024, aud_input_size=128, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n",
    "\n",
    "#         super(Trirar2, self).__init__()\n",
    "        \n",
    "#         self.FrameAndAudioAttention = FrameAndAudioAttention(vid_input_size, aud_input_size, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "#         self.shotFA = AttentionAwareFusion(1024)\n",
    "#         self.audioFA = AttentionAwareFusion(128)\n",
    "#         self.Shot_attention = nn.TransformerEncoderLayer(d_model=1024+128, nhead=4, batch_first=True)\n",
    "#         self.Scorer = nn.Linear(in_features=1024+128, out_features=1)\n",
    "        \n",
    "#         self.ShotsInteraction = MultiheadAttention(input_size=1024+128, output_size=1024+128, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "#         self.sceneFA = AttentionAwareFusion(1024+128)\n",
    "#         self.Scene_attention = nn.TransformerEncoderLayer(d_model=1024+128, nhead=4, batch_first=True)\n",
    "#         self.SceneScorer = nn.Linear(in_features=1024+128, out_features=1)\n",
    "        \n",
    "#         self.probs = nn.Softmax(dim=0)\n",
    "\n",
    "       \n",
    "\n",
    "#         ################# Global Attention ############\n",
    "#         # self.glob_attention = SelfAttention(input_size=input_size, output_size=output_size,\n",
    "#         #                                freq=freq, pos_enc=pos_enc, heads=heads)\n",
    "\n",
    "        \n",
    "#     def forward(self, x, a, shot_boundaries, scene_boundaries, batch_size=5):\n",
    "#         shot_batch=[]\n",
    "#         aud_batch=[]\n",
    "#         batch_mask=[]\n",
    "#         bs_counter=0\n",
    "#         # weighted_value, attn_weights = self.glob_attention(x)  # global attention\n",
    "        \n",
    "#         Shot_embeddings_list=[]\n",
    "#         boundaries_num = len(shot_boundaries)\n",
    "#         start=0\n",
    "#         for shotnum, (_, end) in enumerate(shot_boundaries):\n",
    "#             end=math.ceil(end/15)\n",
    "            \n",
    "#             shot=x[0][start:end]\n",
    "#             audio= a[0][start:end]\n",
    "#             batch_mask.append(torch.zeros(end-start, end-start))\n",
    "#             start = end\n",
    "            \n",
    "#             shot_batch.append(shot)\n",
    "#             aud_batch.append(audio)\n",
    "            \n",
    "#             bs_counter+=1\n",
    "#             if bs_counter != batch_size and shotnum!=boundaries_num-1:\n",
    "#                 continue\n",
    "                \n",
    "#             bs_counter=0\n",
    "            \n",
    "#             shot_batch = pad_sequence(shot_batch, batch_first=True)\n",
    "#             aud_batch = pad_sequence(aud_batch, batch_first=True)\n",
    "            \n",
    "#             pad_length = shot_batch[0].shape[0]\n",
    "#             batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "#             sv, sa = self.FrameAndAudioAttention(shot_batch, aud_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "#             shot_emb = self.shotFA(sv)\n",
    "#             audio_emb = self.audioFA(sa)\n",
    "            \n",
    "#             Shot_embeddings_list.append(torch.hstack((shot_emb,audio_emb)))\n",
    "            \n",
    "#             del(shot_batch)\n",
    "#             del(aud_batch)\n",
    "#             shot_batch=[]\n",
    "#             aud_batch=[]\n",
    "#             del(batch_mask)\n",
    "#             batch_mask=[]\n",
    "            \n",
    "            \n",
    "#         Shot_embeddings = torch.cat(Shot_embeddings_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "#         E2 = self.Shot_attention(Shot_embeddings)\n",
    "        \n",
    "#         P = self.Scorer(E2.squeeze(0))         ##  ???\n",
    "        \n",
    "#         Scene_embeddings_list = []\n",
    "#         batch_mask=[]\n",
    "#         bs_counter=0\n",
    "#         scene_batch = []\n",
    "        \n",
    "#         boundaries_num = len(scene_boundaries)\n",
    "#         for scnnum, (start, end) in enumerate(scene_boundaries):\n",
    "            \n",
    "#             scene=Shot_embeddings[0][start:end+1]\n",
    "#             if end+1-start >5:\n",
    "                \n",
    "#                 if len(scene_batch) !=0:\n",
    "                    \n",
    "#                     scene_batch = pad_sequence(scene_batch, batch_first=True)\n",
    "#                     pad_length = scene_batch[0].shape[0]\n",
    "#                     batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "                    \n",
    "#                     sc = self.ShotsInteraction(scene_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "#                     scene_emb = torch.mean(sc, 1)\n",
    "#                     Scene_embeddings_list.append(scene_emb)\n",
    "                    \n",
    "#                     del(batch_mask)\n",
    "#                     del(scene_batch)\n",
    "#                     batch_mask=[]\n",
    "#                     bs_counter=0\n",
    "#                     scene_batch = []\n",
    "                    \n",
    "#                 sc = self.ShotsInteraction(scene.unsqueeze(0))\n",
    "#                 scene_emb = self.sceneFA(sc)\n",
    "#                 Scene_embeddings_list.append(scene_emb)\n",
    "                \n",
    "#             else:\n",
    "#                 scene_batch.append(scene)\n",
    "#                 batch_mask.append(torch.zeros(end+1-start, end+1-start))\n",
    "#                 bs_counter+=1\n",
    "#                 if bs_counter != batch_size and scnnum!=boundaries_num-1:\n",
    "#                     continue\n",
    "\n",
    "#                 scene_batch = pad_sequence(scene_batch, batch_first=True)\n",
    "#                 pad_length = scene_batch[0].shape[0]\n",
    "#                 batch_mask = torch.stack([ nn.ConstantPad2d((0, pad_length-mask.shape[0], 0, pad_length-mask.shape[0]),1)(mask) for mask in batch_mask])\n",
    "                \n",
    "#                 sc = self.ShotsInteraction(scene_batch, batch_mask.to(next(self.parameters()).device)) \n",
    "#                 scene_emb = torch.mean(sc, 1)\n",
    "#                 Scene_embeddings_list.append(scene_emb)\n",
    "                \n",
    "#                 del(batch_mask)\n",
    "#                 del(scene_batch)\n",
    "#                 batch_mask=[]\n",
    "#                 bs_counter=0\n",
    "#                 scene_batch = []\n",
    "            \n",
    "        \n",
    "#         Scene_embeddings = torch.cat(Scene_embeddings_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "#         E3 = self.Scene_attention(Scene_embeddings)\n",
    "#         S = self.SceneScorer(E3.squeeze(0))\n",
    "        \n",
    "#         for scn, (start, end) in enumerate(scene_boundaries):\n",
    "#             P[start:end+1]+= S[scn]\n",
    "        \n",
    "#         P = self.probs(P.squeeze(0))\n",
    "        \n",
    "       \n",
    "#         return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1dd2f-2e3e-4b84-83d4-c4912371f878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425e701-8b94-4956-affb-fd089e0bc2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890016d5-0631-4660-b175-e802016cec5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d261b-f41d-4d25-938e-0842136e26b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d5f7f-5810-4acd-8b47-db88c99ffdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf5f2f-2cc5-44d8-aa53-dcf30e7b743b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee274ff-6dc9-4347-b012-df0aa5910377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64668162-464e-42e5-957e-e8e9685c2218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d0d8ec-1f0c-4551-af20-e8f24aa5c4c2",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2a8f7-657e-4b95-a064-b9f669b9d351",
   "metadata": {},
   "source": [
    "### Train check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db431aca-0751-4608-a15e-554a69cfc5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train():\n",
    "#         losses=[]\n",
    "#         criterion = nn.MSELoss()\n",
    "        \n",
    "#         with h5py.File('../../Preprocessing/extracted_features/normal/TVSum05s.h5') as d, h5py.File('../../Segmentation/Transnet/transnet_segments/tvsumSegs05s.h5') as scnseg:\n",
    "#             # key = list(d.keys())[0]\n",
    "#             for key in d.keys():\n",
    "#                 print(key)\n",
    "#                 vid_feats= d[key]['features'][...]\n",
    "#                 aud_feats= d[key]['aud_feats'][...]\n",
    "#                 boundaries = d[key]['fchange_points'][...]\n",
    "#                 target = d[key]['gt_probs'][...]\n",
    "#                 scn_boundaries = scnseg[key]['scene_points'][...]\n",
    "#                 n_frames = d[key]['n_frames'][()]\n",
    "#                 target = target.astype(float)\n",
    "                \n",
    "#                 # seq = dataset['features'][...]\n",
    "#                 vid_feats = torch.from_numpy(vid_feats).unsqueeze(0)\n",
    "#                 aud_feats = torch.from_numpy(aud_feats).unsqueeze(0)\n",
    "#                 # target = d['gt_score'][...]\n",
    "#                 target = torch.from_numpy(target)\n",
    "\n",
    "#                 # Min-Max Normalize frame scores\n",
    "#                 target -= target.min()\n",
    "#                 target /= target.max()\n",
    "\n",
    "\n",
    "#                 # if self.hps.use_cuda:\n",
    "#                 #     seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "#                 seq_len = vid_feats.shape[1]\n",
    "#                 # print('video and audio feat:', vid_feats, aud_feats, boundaries, scn_boundaries )\n",
    "#                 P = model(vid_feats, aud_feats, boundaries, scn_boundaries)\n",
    "#                 P=P.reshape(-1)\n",
    "#                 P_frames = get_frame_probs(P, boundaries, n_frames, 'cpu')\n",
    "#                 # print(P.shape, len(boundaries), P)\n",
    "\n",
    "#                 loss_att = 0\n",
    "#                 print(P_frames.shape, target.shape)\n",
    "#                 loss = criterion(P_frames[:len(target)], target.float())\n",
    "#                 loss = loss + loss_att\n",
    "#                 # optimizer.zero_grad()\n",
    "#                 # loss.backward()\n",
    "#                 # optimizer.step()\n",
    "#                 losses.append(float(loss))\n",
    "            \n",
    "#         return np.mean(np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae45bcd2-82da-419e-8b97-c5c0c8c2ab4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# model=Trirar()\n",
    "# parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# optimizer = torch.optim.Adam(parameters, lr=0.00005, weight_decay=0.00001)\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1efa326-6443-4ddf-8119-decdc7ed2e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     def forward(self, vid, aud):\n",
    "        \n",
    "#         vid_embs = []\n",
    "#         aud_embs = []\n",
    "        \n",
    "#         for head in range(self.heads):\n",
    "#             K_v = self.Wk_v[head](vid)\n",
    "#             Q_v = self.Wq_v[head](vid)\n",
    "#             V_v = self.Wv_v[head](vid)\n",
    "            \n",
    "#             K_a = self.Wk_a[head](aud)\n",
    "#             V_a = self.Wv_a[head](aud)\n",
    "            \n",
    "            \n",
    "#             energies_vid = torch.matmul(Q_v, K_v.transpose(1, 2))/np.sqrt(self.vid_output_size//self.heads)\n",
    "#             energies_aud = torch.matmul(Q_v, K_a.transpose(1, 2))/np.sqrt(self.aud_output_size//self.heads)\n",
    "            \n",
    "#             print('mask_check:', energies_aud[0],'\\n softmaxed', self.softmax(energies_aud)[0])\n",
    "#             break\n",
    "            \n",
    "#             vid_att = self.softmax(energies_vid)\n",
    "#             vid_att_weights = self.v_drop(vid_att)\n",
    "#             vid_emb = torch.matmul(vid_att_weights, V_v)\n",
    "            \n",
    "#             aud_att = self.softmax(energies_aud)\n",
    "#             aud_att_weights = self.a_drop(aud_att)\n",
    "#             aud_emb = torch.matmul(aud_att_weights, V_a)\n",
    "            \n",
    "            \n",
    "#             # Save the current head output\n",
    "#             vid_embs.append(vid_emb)\n",
    "#             aud_embs.append(aud_emb)\n",
    "            \n",
    "        \n",
    "#         vid_embs = self.v_W0(torch.cat(vid_embs, dim=-1))\n",
    "        \n",
    "#         vid_embs = self.relu(vid_embs)\n",
    "#         vid_embs =self.v_W2(vid_embs)\n",
    "#         vid_embs =self.v_LN(vid_embs)\n",
    "        \n",
    "#         aud_embs = self.a_W0(torch.cat(aud_embs, dim=-1))\n",
    "#         aud_embs = self.relu(aud_embs)\n",
    "#         aud_embs =self.a_W2(aud_embs)\n",
    "#         aud_embs =self.a_LN(aud_embs)\n",
    "        \n",
    "        \n",
    "#         return vid_embs, aud_embs\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x, a, shot_boundaries):\n",
    "        \n",
    "#         # weighted_value, attn_weights = self.glob_attention(x)  # global attention\n",
    "        \n",
    "#         Shot_embeddings_list=[]\n",
    "#         start=0\n",
    "#         for _, end in shot_boundaries:\n",
    "#             end=math.ceil(end/15)\n",
    "            \n",
    "#             shot=x[0][start:end].unsqueeze(0)\n",
    "#             audio= a[0][start:end].unsqueeze(0)\n",
    "            \n",
    "#             # print('star and stop', start,end)\n",
    "#             start = end\n",
    "            \n",
    "            \n",
    "#             sv, sa = self.FrameAndAudioAttention(shot, audio)\n",
    "#             print('sa:', sa)\n",
    "            \n",
    "#             shot_emb = torch.mean(sv,1)\n",
    "#             audio_emb = torch.mean(sa,1)\n",
    "            \n",
    "#             Shot_embeddings_list.append(torch.hstack((shot_emb,audio_emb)))\n",
    "#             break\n",
    "            \n",
    "#         Shot_embeddings = torch.cat(Shot_embeddings_list, dim=0)\n",
    "        \n",
    "#         E2 = self.Shot_attention(Shot_embeddings.unsqueeze(0))\n",
    "#         P = self.Scorer(E2.squeeze(0))\n",
    "#         P = self.probs(P.squeeze(0))\n",
    "            \n",
    "#         return P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

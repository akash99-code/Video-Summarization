{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1ee811-c01a-4c60-8478-2d1c8a997895",
   "metadata": {},
   "source": [
    "## HMT Tranining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbee983-1aa9-4cd4-9ea5-9c15170c7177",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e69143-7582-4c9e-81de-c9fe7b44c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import json\n",
    "\n",
    "import import_ipynb\n",
    "from Model import HMT\n",
    "\n",
    "from ortools.algorithms import pywrapknapsack_solver\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e3eb6-8ea5-4d5e-b8d1-47b4b00d9e55",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Util modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932d395f-b166-4925-8297-7c44f438fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "def write_json(obj, fpath):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, separators=(',', ': '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b34d5f-84d5-4c92-ac7c-c0c22a76a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_random(keys, num_videos, num_train):\n",
    "    \"\"\"Random split\"\"\"\n",
    "    train_keys, test_keys = [], []\n",
    "    rnd_idxs = np.random.choice(range(num_videos), size=num_train, replace=False)\n",
    "    for key_idx, key in enumerate(keys):\n",
    "        if key_idx in rnd_idxs:\n",
    "            train_keys.append(key)\n",
    "        else:\n",
    "            test_keys.append(key)\n",
    "\n",
    "    assert len(set(train_keys) & set(test_keys)) == 0, \"Error: train_keys and test_keys overlap\"\n",
    "\n",
    "    return train_keys, test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d203a6e8-05a9-426b-9d80-220141b9d33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_splits_filename(splits_filename):\n",
    "    # Parse split file and count number of k_folds\n",
    "    spath, sfname = os.path.split(splits_filename)\n",
    "    sfname, _ = os.path.splitext(sfname)\n",
    "    dataset_name = sfname.split('_')[0]  # Get dataset name e.g. tvsum\n",
    "    dataset_type = sfname.split('_')[1]  # augmentation type e.g. aug\n",
    "\n",
    "    # The keyword 'splits' is used as the filename fields terminator from historical reasons.\n",
    "    if dataset_type == 'splits':\n",
    "        # Split type is not present\n",
    "        dataset_type = ''\n",
    "\n",
    "    # Get number of discrete splits within each split json file\n",
    "    with open(splits_filename, 'r') as sf:\n",
    "        splits = json.load(sf)\n",
    "\n",
    "    return dataset_name, dataset_type, splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83232b22-7b6a-48d1-ad57-48fc76e5e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_probs(shot_probs, cps, n_frames):\n",
    "    \n",
    "    if len(shot_probs) != len(cps):\n",
    "        print('no. of shots does not match')\n",
    "        return\n",
    "    frame_probs = torch.zeros(n_frames, dtype=torch.float32)\n",
    "    n_segs = cps.shape[0]\n",
    "    for seg_idx in range(n_segs):\n",
    "        first, last = cps[seg_idx]\n",
    "        frame_probs[first:last + 1] = shot_probs[seg_idx]\n",
    "        \n",
    "    return frame_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8086414-51b2-4d85-9061-b55a3a2488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname == 'Linear':\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e6ab2d-7669-4beb-9a1d-30d74bfaf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_ortools(values, weights, items, capacity ):\n",
    "    scale = 1000\n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)\n",
    "    values = (values * scale).astype(np.int32)\n",
    "    weights = (weights).astype(np.int32)\n",
    "    capacity = capacity\n",
    "    osolver = pywrapknapsack_solver.KnapsackSolver(pywrapknapsack_solver.KnapsackSolver.KNAPSACK_DYNAMIC_PROGRAMMING_SOLVER,'test')\n",
    "    osolver.Init(values.tolist(), [weights.tolist()], [capacity])\n",
    "    computed_value = osolver.Solve()\n",
    "    packed_items = [x for x in range(0, len(weights))\n",
    "                    if osolver.BestSolutionContains(x)]\n",
    "\n",
    "    return packed_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625eb2e6-0e31-484c-aa05-5cb0dc6fb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(ypred, cps, n_frames, nfps, positions, proportion=0.15, method='knapsack'):\n",
    "    \"\"\"Generate keyshot-based video summary i.e. a binary vector.\n",
    "    Args:\n",
    "    ---------------------------------------------\n",
    "    - ypred: predicted importance scores.\n",
    "    - cps: change points, 2D matrix, each row contains a segment.\n",
    "    - n_frames: original number of frames.\n",
    "    - nfps: number of frames per segment.\n",
    "    - positions: positions of subsampled frames in the original video.\n",
    "    - proportion: length of video summary (compared to original video length).\n",
    "    - method: defines how shots are selected, ['knapsack', 'rank'].\n",
    "    \"\"\"\n",
    "    n_segs = cps.shape[0]\n",
    "    frame_scores = np.zeros((n_frames), dtype=np.float32)\n",
    "    if positions.dtype != int:\n",
    "        positions = positions.astype(np.int32)\n",
    "    if positions[-1] != n_frames:\n",
    "        positions = np.concatenate([positions, [n_frames]])\n",
    "    for i in range(len(positions) - 1):\n",
    "        pos_left, pos_right = positions[i], positions[i+1]\n",
    "        if i == len(ypred):\n",
    "            frame_scores[pos_left:pos_right] = 0\n",
    "        else:\n",
    "            frame_scores[pos_left:pos_right] = ypred[i]\n",
    "\n",
    "    seg_score = []\n",
    "    for seg_idx in range(n_segs):\n",
    "        start, end = int(cps[seg_idx,0]), int(cps[seg_idx,1]+1)\n",
    "        scores = frame_scores[start:end]\n",
    "        seg_score.append(float(scores.mean()))\n",
    "\n",
    "    limits = int(math.floor(n_frames * proportion))\n",
    "\n",
    "    if method == 'knapsack':\n",
    "        #picks = knapsack_dp(seg_score, nfps, n_segs, limits)\n",
    "        picks = knapsack_ortools(seg_score, nfps, n_segs, limits)\n",
    "    elif method == 'rank':\n",
    "        order = np.argsort(seg_score)[::-1].tolist()\n",
    "        picks = []\n",
    "        total_len = 0\n",
    "        for i in order:\n",
    "            if total_len + nfps[i] < limits:\n",
    "                picks.append(i)\n",
    "                total_len += nfps[i]\n",
    "    else:\n",
    "        raise KeyError(\"Unknown method {}\".format(method))\n",
    "\n",
    "    summary = np.zeros((1), dtype=np.float32) # this element should be deleted\n",
    "    for seg_idx in range(n_segs):\n",
    "        nf = nfps[seg_idx]\n",
    "        if seg_idx in picks:\n",
    "            tmp = np.ones((nf), dtype=np.float32)\n",
    "        else:\n",
    "            tmp = np.zeros((nf), dtype=np.float32)\n",
    "        summary = np.concatenate((summary, tmp))\n",
    "\n",
    "    summary = np.delete(summary, 0) # delete the first element\n",
    "    return summary\n",
    "\n",
    "\n",
    "def evaluate_summary(machine_summary, user_summary, eval_metric='avg'):\n",
    "    \"\"\"Compare machine summary with user summary (keyshot-based).\n",
    "    Args:\n",
    "    --------------------------------\n",
    "    machine_summary and user_summary should be binary vectors of ndarray type.\n",
    "    eval_metric = {'avg', 'max'}\n",
    "    'avg' averages results of comparing multiple human summaries.\n",
    "    'max' takes the maximum (best) out of multiple comparisons.\n",
    "    \"\"\"\n",
    "    machine_summary = machine_summary.astype(np.float32)\n",
    "    user_summary = user_summary.astype(np.float32)\n",
    "    n_users,n_frames = user_summary.shape\n",
    "\n",
    "    # binarization\n",
    "    machine_summary[machine_summary > 0] = 1\n",
    "    user_summary[user_summary > 0] = 1\n",
    "\n",
    "    if len(machine_summary) > n_frames:\n",
    "        machine_summary = machine_summary[:n_frames]\n",
    "    elif len(machine_summary) < n_frames:\n",
    "        zero_padding = np.zeros((n_frames - len(machine_summary)))\n",
    "        machine_summary = np.concatenate([machine_summary, zero_padding])\n",
    "\n",
    "    f_scores = []\n",
    "    prec_arr = []\n",
    "    rec_arr = []\n",
    "\n",
    "    for user_idx in range(n_users):\n",
    "        gt_summary = user_summary[user_idx,:]\n",
    "        overlap_duration = (machine_summary * gt_summary).sum()\n",
    "        precision = overlap_duration / (machine_summary.sum() + 1e-8)\n",
    "        recall = overlap_duration / (gt_summary.sum() + 1e-8)\n",
    "        if precision == 0 and recall == 0:\n",
    "            f_score = 0.\n",
    "        else:\n",
    "            f_score = (2 * precision * recall) / (precision + recall)\n",
    "        f_scores.append(f_score)\n",
    "        prec_arr.append(precision)\n",
    "        rec_arr.append(recall)\n",
    "\n",
    "    if eval_metric == 'avg':\n",
    "        final_f_score = np.mean(f_scores)\n",
    "        final_prec = np.mean(prec_arr)\n",
    "        final_rec = np.mean(rec_arr)\n",
    "    elif eval_metric == 'max':\n",
    "        final_f_score = np.max(f_scores)\n",
    "        max_idx = np.argmax(f_scores)\n",
    "        final_prec = prec_arr[max_idx]\n",
    "        final_rec = rec_arr[max_idx]\n",
    "    \n",
    "    return final_f_score, final_prec, final_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf147d9-45af-4a83-ae5e-79fef04e592a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298da162-c1d0-4231-af02-4520cc5ad1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HParameters:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        \n",
    "        self.verbose = args['verbose']\n",
    "        self.use_cuda = args['use_cuda']\n",
    "        self.cuda_device = args['cuda_device']\n",
    "        self.max_summary_length = args['max_summary_length']\n",
    "\n",
    "        self.l2_req = 0.00001\n",
    "        self.lr_epochs = [0]\n",
    "        self.lr = [0.00005]\n",
    "        self.epochs_max = 300\n",
    "        self.train_batch_size = 1\n",
    "\n",
    "        self.dataset=args['dataset']\n",
    "        self.results_path = args['results_path']\n",
    "        self.num_splits = args['num_splits']\n",
    "        self.split_file = args['split_file']\n",
    "        self.train_percent = args['train_percent']\n",
    "        \n",
    "        if 'model_path' in args:\n",
    "            self.model_path = args['model_path']\n",
    "        else:\n",
    "            self.model_path = None\n",
    "        return\n",
    "\n",
    "\n",
    "    def create_split(self):\n",
    "        print(\"Loading dataset from {}\".format(self.dataset))\n",
    "        \n",
    "        with h5py.File(self.dataset, 'r') as dataset:\n",
    "            keys = dataset.keys()\n",
    "            num_videos = len(keys)\n",
    "            num_train = int(math.ceil(num_videos * self.train_percent))\n",
    "            num_test = num_videos - num_train\n",
    "\n",
    "            print(\"Split breakdown: # total videos {}. # train videos {}. # test videos {}\".format(num_videos, num_train, num_test))\n",
    "            splits = []\n",
    "\n",
    "            for split_idx in range(self.num_splits):\n",
    "                train_keys, test_keys = split_random(keys, num_videos, num_train)\n",
    "                splits.append({\n",
    "                    'train_keys': train_keys,\n",
    "                    'test_keys': test_keys,\n",
    "                    })\n",
    "\n",
    "            # saveto = osp.join(self.split_file)\n",
    "            write_json(splits, self.split_file)\n",
    "            print(\"Splits saved to {}\".format(self.split_file))\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        vars = [attr for attr in dir(self) if not callable(getattr(self,attr)) and not (attr.startswith(\"__\") or attr.startswith(\"_\"))]\n",
    "\n",
    "        info_str = ''\n",
    "        for i, var in enumerate(vars):\n",
    "            val = getattr(self, var)\n",
    "            if isinstance(val, Variable):\n",
    "                val = val.data.cpu().numpy().tolist()[0]\n",
    "            info_str += '['+str(i)+'] '+var+': '+str(val)+'\\n'\n",
    "\n",
    "        return info_str\n",
    "    \n",
    "    \n",
    "#     def load_from_args(self, args):\n",
    "#         for key in args:\n",
    "#             val = args[key]\n",
    "#             if val is not None:\n",
    "#                 if hasattr(self, key) and isinstance(getattr(self, key), list):\n",
    "#                     val = val.split()\n",
    "\n",
    "#                 setattr(self, key, val)\n",
    "\n",
    "#     def get_dataset_by_name(self, dataset_name):\n",
    "#         for d in self.datasets:\n",
    "#             if dataset_name in d:\n",
    "#                 return [d]\n",
    "#         return None\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ec2db-e6eb-408f-9442-1e861711badc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "301dbd28-1415-4409-bb4d-821d0a41f72c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, hps: HParameters):\n",
    "        self.hps = hps\n",
    "        self.model = HMT()\n",
    "        self.verbose = True\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.show_every = 1\n",
    "        \n",
    "\n",
    "        \n",
    "    def init_model(self):\n",
    "        if self.hps.model_path:\n",
    "            self.model.load_state_dict(torch.load(self.hps.model_path, map_location=lambda storage, loc: storage))\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.model.apply(weights_init)\n",
    "        \n",
    "   \n",
    "    def train(self, train_keys):\n",
    "        losses=[]\n",
    "        for i, key in enumerate(train_keys):\n",
    "            with h5py.File('../../Preprocessing/extracted_features/normal/TVSum.h5') as d:\n",
    "                vid_feats= d[key]['features'][...]\n",
    "                aud_feats= d[key]['aud_feats'][...]\n",
    "                boundaries = d[key]['change_points'][...]\n",
    "                n_frames = d[key]['n_frames'][()]\n",
    "                target = d[key]['gt_probs'][...]\n",
    "                # target = target.astype(float)\n",
    "                \n",
    "            vid_feats = torch.from_numpy(vid_feats).unsqueeze(0)\n",
    "            aud_feats = torch.from_numpy(aud_feats).unsqueeze(0)\n",
    "            target = torch.Tensor(target)\n",
    "\n",
    "            # Min-Max Normalize frame scores\n",
    "            # target -= target.min()\n",
    "            # target /= target.max()\n",
    "\n",
    "\n",
    "            if self.hps.use_cuda:\n",
    "                seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "            # seq_len = seq.shape[1]\n",
    "            print('Video key:',key, 'video and audio feat shape:', vid_feats.shape, aud_feats.shape)\n",
    "            P = self.model(vid_feats,aud_feats, boundaries)\n",
    "            P = P.reshape(-1)\n",
    "            P_frames = get_frame_probs(P, boundaries, n_frames)\n",
    "        \n",
    "            loss_att = 0\n",
    "            loss = self.criterion(P_frames, target)\n",
    "            print('Loss:', loss)\n",
    "            loss = loss + loss_att\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "            \n",
    "        return np.mean(np.array(losses))\n",
    "\n",
    "    \n",
    "    def video_fscore(self, machine_summary_activations, test_keys, metric='tvsum', att_vecs=None):\n",
    "        eval_metric = 'avg' if metric == 'tvsum' else 'max'\n",
    "\n",
    "        # if results_filename is not None:\n",
    "        #     h5_res = h5py.File(results_filename, 'w')\n",
    "\n",
    "        fms = []\n",
    "        video_scores = []\n",
    "        for key_idx, key in enumerate(test_keys):\n",
    "            \n",
    "            probs = machine_summary_activations[key]\n",
    "\n",
    "\n",
    "            with h5py.File(self.hps.dataset,'r') as d:\n",
    "                cps = d[key]['change_points'][...]\n",
    "                num_frames = d[key]['n_frames'][()]\n",
    "                nfps = d[key]['n_frame_per_seg'][...].tolist()\n",
    "                positions = d[key]['picks'][...]\n",
    "                user_summary = d[key]['user_summary'][...]\n",
    "\n",
    "            machine_summary = generate_summary(probs, cps, num_frames, nfps, positions)\n",
    "            fm, _, _ = evaluate_summary(machine_summary, user_summary, eval_metric)\n",
    "            fms.append(fm)\n",
    "\n",
    "            # Reporting & logging\n",
    "            video_scores.append([key_idx + 1, key, \"{:.1%}\".format(fm)])\n",
    "            \n",
    "        mean_fm = np.mean(fms)\n",
    "        \n",
    "        return mean_fm, video_scores\n",
    "\n",
    "    def validate(self, test_keys):\n",
    "        self.model.eval()\n",
    "        summary = {}\n",
    "        att_vecs = {}\n",
    "        with torch.no_grad():\n",
    "            for i, key in enumerate(test_keys):\n",
    "                with h5py.File(self.hps.dataset) as d:\n",
    "                    seq = d[key]['features'][...]\n",
    "                    \n",
    "                seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "\n",
    "                if self.hps.use_cuda:\n",
    "                    seq = seq.float().cuda()\n",
    "\n",
    "                y, att_vec = self.model(seq, seq.shape[1])\n",
    "                summary[key] = y[0].detach().cpu().numpy()\n",
    "                att_vecs[key] = att_vec.detach().cpu().numpy()\n",
    "\n",
    "        f_score, video_scores = self.video_fscore(summary, test_keys, att_vecs=att_vecs)\n",
    "        return f_score, video_scores\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        print(\"Initializing HMT model and optimizer...\")\n",
    "        self.init_model()\n",
    "        self.model.train()\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            self.criterion = self.criterion.cuda()\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=self.hps.lr[0], weight_decay=self.hps.l2_req)\n",
    "        \n",
    "        lr = self.hps.lr[0]\n",
    "        \n",
    "        f = open(hps.split_file)\n",
    "        splits = json.load(f)\n",
    "        n_folds = len(splits)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        for split in splits:\n",
    "            max_val_fscore = 0\n",
    "            max_val_fscore_epoch = 0\n",
    "            train_keys = split['train_keys']\n",
    "            test_keys = split['test_keys']\n",
    "\n",
    "            epoch_losses=[]\n",
    "            for epoch in range(self.hps.epochs_max):\n",
    "\n",
    "                print(\"Epoch: {0:6}\".format(str(epoch)+\"/\"+str(self.hps.epochs_max)), end='')\n",
    "                self.model.train()\n",
    "\n",
    "                random.shuffle(train_keys) \n",
    "                loss = self.train(train_keys)\n",
    "                epoch_losses.append(np.mean(loss))\n",
    "                \n",
    "                \n",
    "                # # Evaluate test dataset\n",
    "                # val_fscore, video_scores = self.validate(test_keys)\n",
    "                # if max_val_fscore < val_fscore:\n",
    "                #     max_val_fscore = val_fscore\n",
    "                #     max_val_fscore_epoch = epoch\n",
    "                \n",
    "                if epoch%self.show_every==0:\n",
    "                    print(f'Epoch:{epoch}, Loss:{loss}')\n",
    "\n",
    "            # avg_loss = np.array(epoch_losses)\n",
    "            print(\"   Train loss: {0:.05f}\".format(np.mean(np.array(epoch_losses))), end='')\n",
    "            # print('   Test F-score avg/max: {0:0.5}/{1:0.5}'.format(val_fscore, max_val_fscore))\n",
    "\n",
    "            # if self.verbose:\n",
    "            #     video_scores = [[\"No\", \"Video\", \"F-score\"]] + video_scores\n",
    "            #     print_table(video_scores, cell_width=[3,40,8])\n",
    "\n",
    "        # return max_val_fscore, max_val_fscore_epoch\n",
    "        return\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        # Save model weights\n",
    "        filename = name+'_'+str(epoch)+'_'+splitn+'.pth.tar'\n",
    "        torch.save(self.model.state_dict(), os.path.join('models', filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933537a9-9009-4d10-b65b-3d5b8a5f2af1",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee8605c-fa7c-4196-9abd-aadf592dfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'results_path':'training_results.txt',\n",
    "    'num_splits':5,\n",
    "    'split_file':'splits/test_split1.json',\n",
    "    'dataset': '../../Preprocessing/extracted_features/normal/TVSum.h5',\n",
    "    'train_percent':0.8,\n",
    "    'verbose':True,\n",
    "    'use_cuda' : False,\n",
    "    'cuda_device': None,\n",
    "    'max_summary_length': 0.15\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae8df64-01ae-4755-a6e2-39f814bb4035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ../../Preprocessing/extracted_features/normal/TVSum.h5\n",
      "Split breakdown: # total videos 50. # train videos 40. # test videos 10\n",
      "Splits saved to splits/test_split1.json\n"
     ]
    }
   ],
   "source": [
    "hps = HParameters(args)\n",
    "# hps.load_from_args(args.__dict__)\n",
    "hps.create_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb419763-e8a9-4929-b0fa-676846e31fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HMT model and optimizer...\n",
      "Starting training...\n",
      "Epoch: 0/300 Video key: video_35 video and audio feat shape: torch.Size([1, 297, 1024]) torch.Size([1, 297, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_6 video and audio feat shape: torch.Size([1, 644, 1024]) torch.Size([1, 644, 128])\n",
      "Loss: tensor(1.6509e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_3 video and audio feat shape: torch.Size([1, 934, 1024]) torch.Size([1, 934, 128])\n",
      "Loss: tensor(1.0195e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_31 video and audio feat shape: torch.Size([1, 360, 1024]) torch.Size([1, 360, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_36 video and audio feat shape: torch.Size([1, 530, 1024]) torch.Size([1, 530, 128])\n",
      "Loss: tensor(3.6565e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_22 video and audio feat shape: torch.Size([1, 377, 1024]) torch.Size([1, 377, 128])\n",
      "Loss: tensor(5.2293e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_44 video and audio feat shape: torch.Size([1, 286, 1024]) torch.Size([1, 286, 128])\n",
      "Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_27 video and audio feat shape: torch.Size([1, 727, 1024]) torch.Size([1, 727, 128])\n",
      "Loss: tensor(1.0816e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_49 video and audio feat shape: torch.Size([1, 398, 1024]) torch.Size([1, 398, 128])\n",
      "Loss: tensor(5.2154e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_39 video and audio feat shape: torch.Size([1, 277, 1024]) torch.Size([1, 277, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_10 video and audio feat shape: torch.Size([1, 266, 1024]) torch.Size([1, 266, 128])\n",
      "Loss: tensor(8.0829e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_5 video and audio feat shape: torch.Size([1, 221, 1024]) torch.Size([1, 221, 128])\n",
      "Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_25 video and audio feat shape: torch.Size([1, 438, 1024]) torch.Size([1, 438, 128])\n",
      "Loss: tensor(1.8253e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_41 video and audio feat shape: torch.Size([1, 538, 1024]) torch.Size([1, 538, 128])\n",
      "Loss: tensor(2.6804e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_20 video and audio feat shape: torch.Size([1, 416, 1024]) torch.Size([1, 416, 128])\n",
      "Loss: tensor(3.9435e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_40 video and audio feat shape: torch.Size([1, 760, 1024]) torch.Size([1, 760, 128])\n",
      "Loss: tensor(1.2821e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_1 video and audio feat shape: torch.Size([1, 706, 1024]) torch.Size([1, 706, 128])\n",
      "Loss: tensor(1.3155e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_2 video and audio feat shape: torch.Size([1, 312, 1024]) torch.Size([1, 312, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_9 video and audio feat shape: torch.Size([1, 467, 1024]) torch.Size([1, 467, 128])\n",
      "Loss: tensor(3.3254e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_32 video and audio feat shape: torch.Size([1, 253, 1024]) torch.Size([1, 253, 128])\n",
      "Loss: tensor(5.2736e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_29 video and audio feat shape: torch.Size([1, 1168, 1024]) torch.Size([1, 1168, 128])\n",
      "Loss: tensor(4.8063e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_42 video and audio feat shape: torch.Size([1, 395, 1024]) torch.Size([1, 395, 128])\n",
      "Loss: tensor(3.5542e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_50 video and audio feat shape: torch.Size([1, 460, 1024]) torch.Size([1, 460, 128])\n",
      "Loss: tensor(3.4522e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_7 video and audio feat shape: torch.Size([1, 297, 1024]) torch.Size([1, 297, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_13 video and audio feat shape: torch.Size([1, 235, 1024]) torch.Size([1, 235, 128])\n",
      "Loss: tensor(7.5677e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_30 video and audio feat shape: torch.Size([1, 267, 1024]) torch.Size([1, 267, 128])\n",
      "Loss: tensor(5.0566e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_23 video and audio feat shape: torch.Size([1, 375, 1024]) torch.Size([1, 375, 128])\n",
      "Loss: tensor(2.6817e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_15 video and audio feat shape: torch.Size([1, 288, 1024]) torch.Size([1, 288, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_4 video and audio feat shape: torch.Size([1, 480, 1024]) torch.Size([1, 480, 128])\n",
      "Loss: tensor(1.4233e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_26 video and audio feat shape: torch.Size([1, 220, 1024]) torch.Size([1, 220, 128])\n",
      "Loss: tensor(9.1012e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_14 video and audio feat shape: torch.Size([1, 323, 1024]) torch.Size([1, 323, 128])\n",
      "Loss: tensor(4.0447e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_38 video and audio feat shape: torch.Size([1, 196, 1024]) torch.Size([1, 196, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_34 video and audio feat shape: torch.Size([1, 247, 1024]) torch.Size([1, 247, 128])\n",
      "Loss: tensor(3.7442e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_24 video and audio feat shape: torch.Size([1, 290, 1024]) torch.Size([1, 290, 128])\n",
      "Loss: tensor(3.7961e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_21 video and audio feat shape: torch.Size([1, 1293, 1024]) torch.Size([1, 1293, 128])\n",
      "Loss: tensor(3.6638e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_12 video and audio feat shape: torch.Size([1, 900, 1024]) torch.Size([1, 900, 128])\n",
      "Loss: tensor(7.2878e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_46 video and audio feat shape: torch.Size([1, 1020, 1024]) torch.Size([1, 1020, 128])\n",
      "Loss: tensor(3.3080e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_16 video and audio feat shape: torch.Size([1, 635, 1024]) torch.Size([1, 635, 128])\n",
      "Loss: tensor(5.5637e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_47 video and audio feat shape: torch.Size([1, 316, 1024]) torch.Size([1, 316, 128])\n",
      "Loss: tensor(2.4665e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_37 video and audio feat shape: torch.Size([1, 267, 1024]) torch.Size([1, 267, 128])\n",
      "Loss: tensor(4.7035e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch:0, Loss:6.467055330858785e-05\n",
      "Epoch: 1/300 Video key: video_4 video and audio feat shape: torch.Size([1, 480, 1024]) torch.Size([1, 480, 128])\n",
      "Loss: tensor(1.2597e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_2 video and audio feat shape: torch.Size([1, 312, 1024]) torch.Size([1, 312, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_34 video and audio feat shape: torch.Size([1, 247, 1024]) torch.Size([1, 247, 128])\n",
      "Loss: tensor(3.1634e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_41 video and audio feat shape: torch.Size([1, 538, 1024]) torch.Size([1, 538, 128])\n",
      "Loss: tensor(2.5724e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_27 video and audio feat shape: torch.Size([1, 727, 1024]) torch.Size([1, 727, 128])\n",
      "Loss: tensor(9.2107e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_30 video and audio feat shape: torch.Size([1, 267, 1024]) torch.Size([1, 267, 128])\n",
      "Loss: tensor(3.9615e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_22 video and audio feat shape: torch.Size([1, 377, 1024]) torch.Size([1, 377, 128])\n",
      "Loss: tensor(1.2856e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_49 video and audio feat shape: torch.Size([1, 398, 1024]) torch.Size([1, 398, 128])\n",
      "Loss: tensor(4.3667e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_47 video and audio feat shape: torch.Size([1, 316, 1024]) torch.Size([1, 316, 128])\n",
      "Loss: tensor(2.2909e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_42 video and audio feat shape: torch.Size([1, 395, 1024]) torch.Size([1, 395, 128])\n",
      "Loss: tensor(3.1200e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_24 video and audio feat shape: torch.Size([1, 290, 1024]) torch.Size([1, 290, 128])\n",
      "Loss: tensor(4.0530e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_40 video and audio feat shape: torch.Size([1, 760, 1024]) torch.Size([1, 760, 128])\n",
      "Loss: tensor(6.1187e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_46 video and audio feat shape: torch.Size([1, 1020, 1024]) torch.Size([1, 1020, 128])\n",
      "Loss: tensor(3.5539e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_31 video and audio feat shape: torch.Size([1, 360, 1024]) torch.Size([1, 360, 128])\n",
      "Loss: tensor(2.8235e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_29 video and audio feat shape: torch.Size([1, 1168, 1024]) torch.Size([1, 1168, 128])\n",
      "Loss: tensor(4.7717e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_9 video and audio feat shape: torch.Size([1, 467, 1024]) torch.Size([1, 467, 128])\n",
      "Loss: tensor(2.5195e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_38 video and audio feat shape: torch.Size([1, 196, 1024]) torch.Size([1, 196, 128])\n",
      "Loss: tensor(8.5673e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_14 video and audio feat shape: torch.Size([1, 323, 1024]) torch.Size([1, 323, 128])\n",
      "Loss: tensor(3.7436e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_26 video and audio feat shape: torch.Size([1, 220, 1024]) torch.Size([1, 220, 128])\n",
      "Loss: tensor(7.9643e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_35 video and audio feat shape: torch.Size([1, 297, 1024]) torch.Size([1, 297, 128])\n",
      "Loss: tensor(6.6705e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_16 video and audio feat shape: torch.Size([1, 635, 1024]) torch.Size([1, 635, 128])\n",
      "Loss: tensor(5.5761e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_6 video and audio feat shape: torch.Size([1, 644, 1024]) torch.Size([1, 644, 128])\n",
      "Loss: tensor(1.4454e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_20 video and audio feat shape: torch.Size([1, 416, 1024]) torch.Size([1, 416, 128])\n",
      "Loss: tensor(2.5494e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_39 video and audio feat shape: torch.Size([1, 277, 1024]) torch.Size([1, 277, 128])\n",
      "Loss: tensor(9.9626e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_44 video and audio feat shape: torch.Size([1, 286, 1024]) torch.Size([1, 286, 128])\n",
      "Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_3 video and audio feat shape: torch.Size([1, 934, 1024]) torch.Size([1, 934, 128])\n",
      "Loss: tensor(6.4849e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_10 video and audio feat shape: torch.Size([1, 266, 1024]) torch.Size([1, 266, 128])\n",
      "Loss: tensor(3.5511e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_37 video and audio feat shape: torch.Size([1, 267, 1024]) torch.Size([1, 267, 128])\n",
      "Loss: tensor(3.0516e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_15 video and audio feat shape: torch.Size([1, 288, 1024]) torch.Size([1, 288, 128])\n",
      "Loss: tensor(9.4206e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_23 video and audio feat shape: torch.Size([1, 375, 1024]) torch.Size([1, 375, 128])\n",
      "Loss: tensor(2.7253e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_13 video and audio feat shape: torch.Size([1, 235, 1024]) torch.Size([1, 235, 128])\n",
      "Loss: tensor(6.4591e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_36 video and audio feat shape: torch.Size([1, 530, 1024]) torch.Size([1, 530, 128])\n",
      "Loss: tensor(1.3206e-05, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_12 video and audio feat shape: torch.Size([1, 900, 1024]) torch.Size([1, 900, 128])\n",
      "Loss: tensor(7.4926e-06, grad_fn=<MseLossBackward0>)\n",
      "Video key: video_1 video and audio feat shape: torch.Size([1, 706, 1024]) torch.Size([1, 706, 128])\n",
      "Loss: tensor(9.1685e-06, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(hps)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    143\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(train_keys) \n\u001b[1;32m--> 144\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(loss))\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# # Evaluate test dataset\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# val_fscore, video_scores = self.validate(test_keys)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# if max_val_fscore < val_fscore:\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m#     max_val_fscore = val_fscore\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m#     max_val_fscore_epoch = epoch\u001b[39;00m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_keys)\u001b[0m\n\u001b[0;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m loss_att\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(loss))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(hps)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e128302-6514-4aec-9fe2-22af48da162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0cc69-1791-4d60-8568-2d8c9711763b",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

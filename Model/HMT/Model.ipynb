{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cefdcb-ac5d-45f2-9963-2487f18caea4",
   "metadata": {},
   "source": [
    "## HMT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dee9b-147b-486a-8a31-dac71c59db83",
   "metadata": {},
   "source": [
    "**packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6911d90f-bdb7-482f-8e85-0dc2b420ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import *\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dea82-e275-4a1d-9387-4f5a6f805e16",
   "metadata": {},
   "source": [
    "#### MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b1e3f1-7a23-4c7a-a5ec-f2add7a4514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_size=1024, output_size=1024, freq=10000, heads=1, pos_enc=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "#         self.permitted_encodings = [\"absolute\", \"relative\"]\n",
    "#         if pos_enc is not None:\n",
    "#             pos_enc = pos_enc.lower()\n",
    "#             assert pos_enc in self.permitted_encodings, f\"Supported encodings: {*self.permitted_encodings,}\"\n",
    "\n",
    "        self.heads = heads\n",
    "        self.pos_enc = pos_enc\n",
    "        self.freq = freq\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size \n",
    "        self.Wq, self.Wk, self.Wv = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.heads):\n",
    "            self.Wk.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            self.Wq.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            self.Wv.append(nn.Linear(in_features=input_size, out_features=output_size//heads, bias=False))\n",
    "            \n",
    "        self.W0 = nn.Linear(in_features=input_size, out_features=input_size, bias=False)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.W2 = nn.Linear(in_features=input_size, out_features=input_size, bias=False)\n",
    "        self.LN = nn.LayerNorm(normalized_shape=self.W2.out_features, eps=1e-6)\n",
    "    \n",
    "    \n",
    "########################################## Positional Encoding Code ###############################\n",
    "#     def getAbsolutePosition(self, T):\n",
    "#         \"\"\"Calculate the sinusoidal positional encoding based on the absolute position of each considered frame.\n",
    "#         Based on 'Attention is all you need' paper (https://arxiv.org/abs/1706.03762)\n",
    "#         :param int T: Number of frames contained in Q, K and V\n",
    "#         :return: Tensor with shape [T, T]\n",
    "#         \"\"\"\n",
    "#         freq = self.freq\n",
    "#         d = self.input_size\n",
    "\n",
    "#         pos = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "#         i = torch.tensor([k for k in range(T//2)], device=self.out.weight.device)\n",
    "\n",
    "#         # Reshape tensors each pos_k for each i indices\n",
    "#         pos = pos.reshape(pos.shape[0], 1)\n",
    "#         pos = pos.repeat_interleave(i.shape[0], dim=1)\n",
    "#         i = i.repeat(pos.shape[0], 1)\n",
    "\n",
    "#         AP = torch.zeros(T, T, device=self.out.weight.device)\n",
    "#         AP[pos, 2*i] = torch.sin(pos / freq ** ((2 * i) / d))\n",
    "#         AP[pos, 2*i+1] = torch.cos(pos / freq ** ((2 * i) / d))\n",
    "#         return AP\n",
    "\n",
    "#     def getRelativePosition(self, T):\n",
    "#         \"\"\"Calculate the sinusoidal positional encoding based on the relative position of each considered frame.\n",
    "#         r_pos calculations as here: https://theaisummer.com/positional-embeddings/\n",
    "#         :param int T: Number of frames contained in Q, K and V\n",
    "#         :return: Tensor with shape [T, T]\n",
    "#         \"\"\"\n",
    "#         freq = self.freq\n",
    "#         d = 2 * T\n",
    "#         min_rpos = -(T - 1)\n",
    "\n",
    "#         i = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "#         j = torch.tensor([k for k in range(T)], device=self.out.weight.device)\n",
    "\n",
    "#         # Reshape tensors each i for each j indices\n",
    "#         i = i.reshape(i.shape[0], 1)\n",
    "#         i = i.repeat_interleave(i.shape[0], dim=1)\n",
    "#         j = j.repeat(i.shape[0], 1)\n",
    "\n",
    "#         # Calculate the relative positions\n",
    "#         r_pos = j - i - min_rpos\n",
    "\n",
    "#         RP = torch.zeros(T, T, device=self.out.weight.device)\n",
    "#         idx = torch.tensor([k for k in range(T//2)], device=self.out.weight.device)\n",
    "#         RP[:, 2*idx] = torch.sin(r_pos[:, 2*idx] / freq ** ((i[:, 2*idx] + j[:, 2*idx]) / d))\n",
    "#         RP[:, 2*idx+1] = torch.cos(r_pos[:, 2*idx+1] / freq ** ((i[:, 2*idx+1] + j[:, 2*idx+1]) / d))\n",
    "#         return RP\n",
    "###################################################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embs = []\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            K = self.Wk[head](x)\n",
    "            Q = self.Wq[head](x)\n",
    "            V = self.Wv[head](x)\n",
    "            \n",
    "\n",
    "            # Q *= 0.06                            # scale factor\n",
    "            \n",
    "            energies = torch.matmul(Q, K.transpose(1, 2))/np.sqrt(self.output_size//self.heads)\n",
    "            \n",
    "            ######### Positional Encoding skipped ##############\n",
    "            # if self.pos_enc is not None:\n",
    "            #     if self.pos_enc == \"absolute\":\n",
    "            #         AP = self.getAbsolutePosition(T=energies.shape[0])\n",
    "            #         energies = energies + AP\n",
    "            #     elif self.pos_enc == \"relative\":\n",
    "            #         RP = self.getRelativePosition(T=energies.shape[0])\n",
    "            #         energies = energies + RP\n",
    "            ###########################################\n",
    "            \n",
    "            \n",
    "            _att = self.softmax(energies)\n",
    "            att_weights = self.drop(_att)\n",
    "            emb = torch.matmul(att_weights, V)\n",
    "            \n",
    "            # Save the current head output\n",
    "            embs.append(emb)\n",
    "            \n",
    "        embs = self.W0(torch.cat(embs, dim=-1))\n",
    "        embs = self.relu(embs)\n",
    "        embs = self.W2(embs)\n",
    "        embs = self.LN(embs)\n",
    "        \n",
    "        return embs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a48092-969c-4f86-a845-d7b021bce9c8",
   "metadata": {},
   "source": [
    "### Frame and Audio Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "242a7f9a-049c-4f9c-9f0a-c15a657e3be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class FrameAndAudioAttention(nn.Module):\n",
    "    def __init__(self, vid_input_size=1024, aud_input_size=128, freq=10000, heads=1, pos_enc=None):\n",
    "        \n",
    "        super(FrameAndAudioAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.pos_enc = pos_enc\n",
    "        self.freq = freq\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        ##### Attention for Frames\n",
    "        self.vid_input_size = vid_input_size\n",
    "        self.vid_output_size = vid_input_size \n",
    "        self.Wq_v, self.Wk_v, self.Wv_v = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(self.heads):\n",
    "            self.Wk_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wq_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wv_v.append(nn.Linear(in_features=vid_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            \n",
    "        self.v_W0 = nn.Linear(in_features=vid_input_size, out_features=vid_input_size, bias=False)\n",
    "        self.v_drop = nn.Dropout(p=0.5)\n",
    "        self.v_W2 = nn.Linear(in_features=vid_input_size, out_features=vid_input_size, bias=False)\n",
    "        self.v_LN = nn.LayerNorm(normalized_shape=self.v_W2.out_features, eps=1e-6)\n",
    "        \n",
    "        \n",
    "        ##### Attention for Audios\n",
    "        self.aud_input_size = aud_input_size\n",
    "        self.aud_output_size = aud_input_size\n",
    "        self.Wk_a, self.Wv_a = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(self.heads):\n",
    "            self.Wk_a.append(nn.Linear(in_features=aud_input_size, out_features=vid_input_size//heads, bias=False))\n",
    "            self.Wv_a.append(nn.Linear(in_features=aud_input_size, out_features=aud_input_size//heads, bias=False))\n",
    "            \n",
    "        self.a_W0 = nn.Linear(in_features=aud_input_size, out_features=aud_input_size, bias=False)\n",
    "        self.a_drop = nn.Dropout(p=0.5)\n",
    "        self.a_W2 = nn.Linear(in_features=aud_input_size, out_features=aud_input_size, bias=False)\n",
    "        self.a_LN = nn.LayerNorm(normalized_shape=self.a_W2.out_features, eps=1e-6)\n",
    "\n",
    "        \n",
    "    def forward(self, vid, aud):\n",
    "        \n",
    "        vid_embs = []\n",
    "        aud_embs = []\n",
    "        \n",
    "        for head in range(self.heads):\n",
    "            K_v = self.Wk_v[head](vid)\n",
    "            Q_v = self.Wq_v[head](vid)\n",
    "            V_v = self.Wv_v[head](vid)\n",
    "            \n",
    "            K_a = self.Wk_a[head](aud)\n",
    "            V_a = self.Wv_a[head](aud)\n",
    "            \n",
    "            \n",
    "            energies_vid = torch.matmul(Q_v, K_v.transpose(1, 2))/np.sqrt(self.vid_output_size//self.heads)\n",
    "            energies_aud = torch.matmul(Q_v, K_a.transpose(1, 2))/np.sqrt(self.aud_output_size//self.heads)\n",
    "\n",
    "            vid_att = self.softmax(energies_vid)\n",
    "            vid_att_weights = self.v_drop(vid_att)\n",
    "            vid_emb = torch.matmul(vid_att_weights, V_v)\n",
    "            \n",
    "            aud_att = self.softmax(energies_aud)\n",
    "            aud_att_weights = self.a_drop(aud_att)\n",
    "            aud_emb = torch.matmul(aud_att_weights, V_a)\n",
    "            \n",
    "            \n",
    "            # Save the current head output\n",
    "            vid_embs.append(vid_emb)\n",
    "            aud_embs.append(aud_emb)\n",
    "            \n",
    "        \n",
    "        vid_embs = self.v_W0(torch.cat(vid_embs, dim=-1))\n",
    "        \n",
    "        vid_embs = self.relu(vid_embs)\n",
    "        vid_embs =self.v_W2(vid_embs)\n",
    "        vid_embs =self.v_LN(vid_embs)\n",
    "        \n",
    "        aud_embs = self.a_W0(torch.cat(aud_embs, dim=-1))\n",
    "        aud_embs = self.relu(aud_embs)\n",
    "        aud_embs =self.a_W2(aud_embs)\n",
    "        aud_embs =self.a_LN(aud_embs)\n",
    "        \n",
    "        \n",
    "        return vid_embs, aud_embs\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb8f0a-0a31-4c4d-ab8a-f3693c6bb646",
   "metadata": {},
   "source": [
    "### Hierarchical Multimodal Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aa343fa-96a3-485f-a5d7-a7d7a38aed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HMT(nn.Module):\n",
    "    def __init__(self, vid_input_size=1024, aud_input_size=128, freq=10000, pos_enc=None, num_segments=None, heads=1, fusion=None):\n",
    "\n",
    "        super(HMT, self).__init__()\n",
    "        \n",
    "        self.FrameAndAudioAttention = FrameAndAudioAttention(vid_input_size, aud_input_size, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "        self.Shot_attention = MultiheadAttention(input_size=1024+128, output_size=1024+128, freq=freq, pos_enc=pos_enc, heads=4)\n",
    "        self.Scorer = nn.Linear(in_features=1024+128, out_features=1)\n",
    "        self.probs = nn.Softmax(dim=0)\n",
    "       \n",
    "\n",
    "        ################# Global Attention ############\n",
    "        # self.glob_attention = SelfAttention(input_size=input_size, output_size=output_size,\n",
    "        #                                freq=freq, pos_enc=pos_enc, heads=heads)\n",
    "\n",
    "        \n",
    "    def forward(self, x, a, shot_boundaries):\n",
    "        \n",
    "        # weighted_value, attn_weights = self.glob_attention(x)  # global attention\n",
    "        \n",
    "        Shot_embeddings_list=[]\n",
    "        start=0\n",
    "        for _, end in shot_boundaries:\n",
    "            end=math.ceil(end/15)\n",
    "            \n",
    "            shot=x[0][start:end].unsqueeze(0)\n",
    "            audio= a[0][start:end].unsqueeze(0)\n",
    "            \n",
    "            # print('star and stop', start,end)\n",
    "            start = end\n",
    "            \n",
    "            \n",
    "            sv, sa = self.FrameAndAudioAttention(shot, audio)\n",
    "            \n",
    "            shot_emb = torch.mean(sv,1)\n",
    "            audio_emb = torch.mean(sa,1)\n",
    "            \n",
    "            Shot_embeddings_list.append(torch.hstack((shot_emb,audio_emb)))\n",
    "            \n",
    "        Shot_embeddings = torch.cat(Shot_embeddings_list, dim=0)\n",
    "        \n",
    "        E2 = self.Shot_attention(Shot_embeddings.unsqueeze(0))\n",
    "        P = self.Scorer(E2.squeeze(0))\n",
    "        P = self.probs(P.squeeze(0))\n",
    "            \n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efa326-6443-4ddf-8119-decdc7ed2e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398be8f7-590f-4f28-93a1-5665ef040f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7d450-7929-4d3c-a7fb-4506b0dc94cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d2dfa-ecc0-4437-a888-7fdbaa402031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffacfe-1134-4af3-b257-e54994b4cd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80283436-e160-42c8-b718-af8240907e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1fe08-7367-4a72-8021-fafe1d32d004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127fd93-b596-43d4-bf86-a2cc2449f1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d0d8ec-1f0c-4551-af20-e8f24aa5c4c2",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db431aca-0751-4608-a15e-554a69cfc5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "        losses=[]\n",
    "        with h5py.File('../../Preprocessing/extracted_features/normal/TVSum.h5') as d:\n",
    "            key = list(d.keys())[0]\n",
    "            vid_feats= d[key]['features'][...]\n",
    "            aud_feats= d[key]['aud_feats'][...]\n",
    "            boundaries = d[key]['change_points'][...]\n",
    "            target = d[key]['gt_probs'][...]\n",
    "            # target = target.astype(float)\n",
    "                \n",
    "        # seq = dataset['features'][...]\n",
    "        vid_feats = torch.from_numpy(vid_feats).unsqueeze(0)\n",
    "        aud_feats = torch.from_numpy(aud_feats).unsqueeze(0)\n",
    "        # target = dataset['gtscore'][...]\n",
    "        # target = torch.from_numpy(target).unsqueeze(0)\n",
    "\n",
    "        # Min-Max Normalize frame scores\n",
    "        # target -= target.min()\n",
    "        # target /= target.max()\n",
    "\n",
    "\n",
    "#         if self.hps.use_cuda:\n",
    "#             seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "        # seq_len = seq.shape[1]\n",
    "        print('video and audio feat:', vid_feats.shape, aud_feats.shape)\n",
    "        P = model(vid_feats, aud_feats, boundaries)\n",
    "        # print(P)\n",
    "        \n",
    "#         loss_att = 0\n",
    "        print(P.reshape(-1))\n",
    "#         loss = self.criterion(P, target.float())\n",
    "#         loss = loss + loss_att\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         losses.append(float(loss))\n",
    "            \n",
    "        return np.mean(np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae45bcd2-82da-419e-8b97-c5c0c8c2ab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video and audio feat: torch.Size([1, 706, 1024]) torch.Size([1, 706, 128])\n",
      "stack shape: torch.Size([1, 184, 1152])\n",
      "tensor([0.0052, 0.0050, 0.0051, 0.0062, 0.0052, 0.0051, 0.0055, 0.0060, 0.0051,\n",
      "        0.0055, 0.0054, 0.0057, 0.0057, 0.0057, 0.0053, 0.0052, 0.0054, 0.0057,\n",
      "        0.0053, 0.0052, 0.0050, 0.0055, 0.0052, 0.0053, 0.0051, 0.0053, 0.0051,\n",
      "        0.0061, 0.0061, 0.0052, 0.0057, 0.0049, 0.0052, 0.0056, 0.0048, 0.0059,\n",
      "        0.0056, 0.0057, 0.0058, 0.0053, 0.0057, 0.0057, 0.0053, 0.0050, 0.0047,\n",
      "        0.0051, 0.0055, 0.0052, 0.0057, 0.0057, 0.0055, 0.0051, 0.0054, 0.0049,\n",
      "        0.0055, 0.0055, 0.0056, 0.0052, 0.0058, 0.0054, 0.0058, 0.0054, 0.0052,\n",
      "        0.0051, 0.0053, 0.0053, 0.0054, 0.0054, 0.0050, 0.0058, 0.0057, 0.0049,\n",
      "        0.0052, 0.0061, 0.0051, 0.0055, 0.0049, 0.0056, 0.0053, 0.0053, 0.0051,\n",
      "        0.0053, 0.0059, 0.0060, 0.0051, 0.0056, 0.0052, 0.0053, 0.0056, 0.0059,\n",
      "        0.0058, 0.0055, 0.0049, 0.0051, 0.0053, 0.0053, 0.0054, 0.0063, 0.0054,\n",
      "        0.0056, 0.0054, 0.0056, 0.0058, 0.0056, 0.0053, 0.0054, 0.0059, 0.0055,\n",
      "        0.0054, 0.0054, 0.0055, 0.0051, 0.0055, 0.0050, 0.0051, 0.0058, 0.0058,\n",
      "        0.0060, 0.0053, 0.0051, 0.0053, 0.0058, 0.0054, 0.0058, 0.0058, 0.0057,\n",
      "        0.0053, 0.0055, 0.0055, 0.0053, 0.0058, 0.0055, 0.0053, 0.0062, 0.0055,\n",
      "        0.0054, 0.0054, 0.0048, 0.0055, 0.0056, 0.0052, 0.0055, 0.0053, 0.0058,\n",
      "        0.0058, 0.0052, 0.0055, 0.0055, 0.0053, 0.0055, 0.0058, 0.0050, 0.0051,\n",
      "        0.0055, 0.0062, 0.0054, 0.0055, 0.0058, 0.0054, 0.0050, 0.0053, 0.0050,\n",
      "        0.0052, 0.0058, 0.0050, 0.0058, 0.0051, 0.0049, 0.0051, 0.0054, 0.0051,\n",
      "        0.0054, 0.0057, 0.0054, 0.0054, 0.0054, 0.0055, 0.0054, 0.0048, 0.0056,\n",
      "        0.0056, 0.0060, 0.0058, 0.0057], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=HMT()\n",
    "# train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

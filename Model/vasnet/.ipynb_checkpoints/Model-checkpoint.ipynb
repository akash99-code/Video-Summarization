{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609dee9b-147b-486a-8a31-dac71c59db83",
   "metadata": {},
   "source": [
    "**packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6911d90f-bdb7-482f-8e85-0dc2b420ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c19661-e614-42ab-8ca9-b9946ec77b48",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60721b32-6f63-470e-9082-db85a29233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "    \n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, apperture=-1, ignore_itself=False, input_size=1024, output_size=1024):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.apperture = apperture\n",
    "        self.ignore_itself = ignore_itself\n",
    "\n",
    "        self.m = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.K = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
    "        self.Q = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
    "        self.V = nn.Linear(in_features=self.m, out_features=self.output_size, bias=False)\n",
    "        self.output_linear = nn.Linear(in_features=self.output_size, out_features=self.m, bias=False)\n",
    "\n",
    "        self.drop50 = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]  # sequence length\n",
    "\n",
    "        K = self.K(x)  # ENC (n x m) => (n x H) H= hidden size\n",
    "        Q = self.Q(x)  # ENC (n x m) => (n x H) H= hidden size\n",
    "        V = self.V(x)\n",
    "\n",
    "        Q *= 0.06\n",
    "        logits = torch.matmul(Q, K.transpose(1,0))\n",
    "\n",
    "        if self.ignore_itself:\n",
    "            # Zero the diagonal activations (a distance of each frame with itself)\n",
    "            logits[torch.eye(n).byte()] = -float(\"Inf\")\n",
    "\n",
    "        if self.apperture > 0:\n",
    "            # Set attention to zero to frames further than +/- apperture from the current one\n",
    "            onesmask = torch.ones(n, n)\n",
    "            trimask = torch.tril(onesmask, -self.apperture) + torch.triu(onesmask, self.apperture)\n",
    "            logits[trimask == 1] = -float(\"Inf\")\n",
    "\n",
    "        att_weights_ = nn.functional.softmax(logits, dim=-1)\n",
    "        weights = self.drop50(att_weights_)\n",
    "        y = torch.matmul(V.transpose(1,0), weights).transpose(1,0)\n",
    "        y = self.output_linear(y)\n",
    "\n",
    "        return y, att_weights_\n",
    "\n",
    "\n",
    "\n",
    "class VASNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VASNet, self).__init__()\n",
    "\n",
    "        self.m = 1024 # cnn features size\n",
    "        self.hidden_size = 1024\n",
    "\n",
    "        self.att = SelfAttention(input_size=self.m, output_size=self.m)\n",
    "        self.ka = nn.Linear(in_features=self.m, out_features=1024)\n",
    "        self.kb = nn.Linear(in_features=self.ka.out_features, out_features=1024)\n",
    "        self.kc = nn.Linear(in_features=self.kb.out_features, out_features=1024)\n",
    "        self.kd = nn.Linear(in_features=self.ka.out_features, out_features=1)\n",
    "\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop50 = nn.Dropout(0.5)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.layer_norm_y = LayerNorm(self.m)\n",
    "        self.layer_norm_ka = LayerNorm(self.ka.out_features)\n",
    "\n",
    "\n",
    "    def forward(self, x, seq_len):\n",
    "\n",
    "        m = x.shape[2] # Feature size\n",
    "\n",
    "        # Place the video frames to the batch dimension to allow for batch arithm. operations.\n",
    "        # Assumes input batch size = 1.\n",
    "        x = x.view(-1, m)\n",
    "        y, att_weights_ = self.att(x)\n",
    "\n",
    "        y = y + x\n",
    "        y = self.drop50(y)\n",
    "        y = self.layer_norm_y(y)\n",
    "\n",
    "        # Frame level importance score regression\n",
    "        # Two layer NN\n",
    "        y = self.ka(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.drop50(y)\n",
    "        y = self.layer_norm_ka(y)\n",
    "\n",
    "        y = self.kd(y)\n",
    "        y = self.sig(y)\n",
    "        y = y.view(1, -1)\n",
    "\n",
    "        return y, att_weights_\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb01d8-a8d3-4589-a437-d72664a87434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b37c6-89ae-41b4-abe6-86c9efb1d756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333de65-5c85-41cb-854d-5a7074036d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f1352-8cdc-4c23-884e-b205def691e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06d63e1-8066-414c-a1b2-59788bada56a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### )) Parameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec39c4-0fd0-4e18-9b8a-2209371197e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParameters:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        \n",
    "        self.verbose = args['verbose']\n",
    "        self.use_cuda = args['use_cuda']\n",
    "        self.cuda_device = args['cuda_device']\n",
    "        self.max_summary_length = args['max_summary_length']\n",
    "\n",
    "        self.l2_req = 0.00001\n",
    "        self.lr_epochs = [0]\n",
    "        self.lr = [0.00005]\n",
    "        self.epochs_max = 300\n",
    "        self.train_batch_size = 1\n",
    "\n",
    "        self.model = args['model']\n",
    "        self.datasets=args['datasets']\n",
    "        self.splits = args['splits']\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_dataset_by_name(self, dataset_name):\n",
    "        for d in self.datasets:\n",
    "            if dataset_name in d:\n",
    "                return [d]\n",
    "        return None\n",
    "\n",
    "    def load_from_args(self, args):\n",
    "        for key in args:\n",
    "            val = args[key]\n",
    "            if val is not None:\n",
    "                if hasattr(self, key) and isinstance(getattr(self, key), list):\n",
    "                    val = val.split()\n",
    "\n",
    "                setattr(self, key, val)\n",
    "\n",
    "    def __str__(self):\n",
    "        vars = [attr for attr in dir(self) if not callable(getattr(self,attr)) and not (attr.startswith(\"__\") or attr.startswith(\"_\"))]\n",
    "\n",
    "        info_str = ''\n",
    "        for i, var in enumerate(vars):\n",
    "            val = getattr(self, var)\n",
    "            if isinstance(val, Variable):\n",
    "                val = val.data.cpu().numpy().tolist()[0]\n",
    "            info_str += '['+str(i)+'] '+var+': '+str(val)+'\\n'\n",
    "\n",
    "        return info_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90cddfa-e24b-43f7-9a99-26baf9e4f381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c6025a-1bd5-4bc8-9e26-07f46137c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AONet:\n",
    "\n",
    "    def __init__(self, hps: HParameters):\n",
    "        self.hps = hps\n",
    "        self.model = None\n",
    "        self.log_file = None\n",
    "        self.verbose = hps.verbose\n",
    "\n",
    "\n",
    "    def initialize(self, cuda_device=None):\n",
    "        rnd_seed = 12345\n",
    "        random.seed(rnd_seed)\n",
    "        np.random.seed(rnd_seed)\n",
    "        torch.manual_seed(rnd_seed)\n",
    "\n",
    "        self.model = VASNet()\n",
    "        self.model.eval()\n",
    "        # self.model.apply(weights_init)##?\n",
    "        #print(self.model)\n",
    "\n",
    "        cuda_device = cuda_device or self.hps.cuda_device\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            print(\"Setting CUDA device: \",cuda_device)\n",
    "            torch.cuda.set_device(cuda_device)\n",
    "            torch.cuda.manual_seed(rnd_seed)\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def load_datasets(self, datasets = None):\n",
    "        \"\"\"\n",
    "        Loads all h5 datasets from the datasets list into a dictionary self.dataset\n",
    "        referenced by their base filename\n",
    "        :param datasets:  List of dataset filenames\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = self.hps.datasets\n",
    "\n",
    "        datasets_dict = {}\n",
    "        for dataset in datasets:\n",
    "            _, base_filename = os.path.split(dataset)\n",
    "            base_filename, _ = os.path.splitext(base_filename)\n",
    "            print(\"Loading:\", dataset)\n",
    "            # dataset_name = base_filename.split('_')[2]\n",
    "            # print(\"\\tDataset name:\", dataset_name)\n",
    "            # datasets_dict[base_filename] = h5py.File(dataset, 'r')\n",
    "            datasets_dict[base_filename] = dataset\n",
    "\n",
    "        self.datasets_dict = datasets_dict\n",
    "        \n",
    "        return datasets_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_model(self, model_filename):\n",
    "        self.model.load_state_dict(torch.load(model_filename, map_location=lambda storage, loc: storage))\n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def fix_keys(self, keys, dataset_name = None):\n",
    "#         \"\"\"\n",
    "#         :param keys:\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         # dataset_name = None\n",
    "#         if len(self.datasets) == 1:\n",
    "#             dataset_name = next(iter(self.datasets))\n",
    "\n",
    "#         keys_out = []\n",
    "#         for key in keys:\n",
    "#             t = key.split('/')\n",
    "#             if len(t) != 2:\n",
    "#                 assert dataset_name is not None, \"ERROR dataset name in some keys is missing but there are multiple dataset {} to choose from\".format(len(self.datasets))\n",
    "\n",
    "#                 key_name = dataset_name+'/'+key\n",
    "#                 keys_out.append(key_name)\n",
    "#             else:\n",
    "#                 keys_out.append(key)\n",
    "\n",
    "#         return keys_out\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     def load_split_file(self, splits_file):\n",
    "\n",
    "#         self.dataset_name, self.dataset_type, self.splits = parse_splits_filename(splits_file)\n",
    "#         n_folds = len(self.splits)\n",
    "#         self.split_file = splits_file\n",
    "#         print(\"Loading splits from: \",splits_file)\n",
    "\n",
    "#         return n_folds\n",
    "\n",
    "\n",
    "#     def select_split(self, split_id):\n",
    "#         print(\"Selecting split: \",split_id)\n",
    "\n",
    "#         self.split_id = split_id\n",
    "#         n_folds = len(self.splits)\n",
    "#         assert self.split_id < n_folds, \"split_id (got {}) exceeds {}\".format(self.split_id, n_folds)\n",
    "\n",
    "#         split = self.splits[self.split_id]\n",
    "#         self.train_keys = split['train_keys']\n",
    "#         self.test_keys = split['test_keys']\n",
    "\n",
    "#         dataset_filename = self.hps.get_dataset_by_name(self.dataset_name)[0]\n",
    "#         _,dataset_filename = os.path.split(dataset_filename)\n",
    "#         dataset_filename,_ = os.path.splitext(dataset_filename)\n",
    "#         self.train_keys = self.fix_keys(self.train_keys, dataset_filename)\n",
    "#         self.test_keys = self.fix_keys(self.test_keys, dataset_filename)\n",
    "#         return\n",
    "\n",
    "\n",
    "\n",
    "#     def get_data(self, key):\n",
    "#         key_parts = key.split('/')\n",
    "#         assert len(key_parts) == 2, \"ERROR. Wrong key name: \"+key\n",
    "#         dataset, key = key_parts\n",
    "#         return self.datasets[dataset][key]\n",
    "\n",
    "#     def lookup_weights_file(self, data_path):\n",
    "#         dataset_type_str = '' if self.dataset_type == '' else self.dataset_type + '_'\n",
    "#         weights_filename = data_path + '/models/{}_{}splits_{}_*.tar.pth'.format(self.dataset_name, dataset_type_str, self.split_id)\n",
    "#         weights_filename = glob.glob(weights_filename)\n",
    "#         if len(weights_filename) == 0:\n",
    "#             print(\"Couldn't find model weights: \", weights_filename)\n",
    "#             return ''\n",
    "\n",
    "#         # Get the first weights filename in the dir\n",
    "#         weights_filename = weights_filename[0]\n",
    "#         splits_file = data_path + '/splits/{}_{}splits.json'.format(self.dataset_name, dataset_type_str)\n",
    "\n",
    "#         return weights_filename, splits_file\n",
    "\n",
    "\n",
    "    def train(self, output_dir='EX-0'):\n",
    "\n",
    "        print(\"Initializing VASNet model and optimizer...\")\n",
    "        self.model.train()\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=self.hps.lr[0], weight_decay=self.hps.l2_req)\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        max_val_fscore = 0\n",
    "        max_val_fscore_epoch = 0\n",
    "        train_keys = self.train_keys[:]\n",
    "\n",
    "        lr = self.hps.lr[0]\n",
    "        for epoch in range(self.hps.epochs_max):\n",
    "\n",
    "            print(\"Epoch: {0:6}\".format(str(epoch)+\"/\"+str(self.hps.epochs_max)), end='')\n",
    "            self.model.train()\n",
    "            avg_loss = []\n",
    "\n",
    "            random.shuffle(train_keys)\n",
    "\n",
    "            for i, key in enumerate(train_keys):\n",
    "                dataset = self.get_data(key)\n",
    "                seq = dataset['features'][...]\n",
    "                seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "                target = dataset['gtscore'][...]\n",
    "                target = torch.from_numpy(target).unsqueeze(0)\n",
    "\n",
    "                # Normalize frame scores\n",
    "                target -= target.min()\n",
    "                target /= target.max()\n",
    "\n",
    "                if self.hps.use_cuda:\n",
    "                    seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "                seq_len = seq.shape[1]\n",
    "                y, _ = self.model(seq,seq_len)\n",
    "                loss_att = 0\n",
    "\n",
    "                loss = criterion(y, target)\n",
    "                # loss2 = y.sum()/seq_len\n",
    "                loss = loss + loss_att\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_loss.append([float(loss), float(loss_att)])\n",
    "\n",
    "            # Evaluate test dataset\n",
    "            val_fscore, video_scores = self.eval(self.test_keys)\n",
    "            if max_val_fscore < val_fscore:\n",
    "                max_val_fscore = val_fscore\n",
    "                max_val_fscore_epoch = epoch\n",
    "\n",
    "            avg_loss = np.array(avg_loss)\n",
    "            print(\"   Train loss: {0:.05f}\".format(np.mean(avg_loss[:, 0])), end='')\n",
    "            print('   Test F-score avg/max: {0:0.5}/{1:0.5}'.format(val_fscore, max_val_fscore))\n",
    "\n",
    "            if self.verbose:\n",
    "                video_scores = [[\"No\", \"Video\", \"F-score\"]] + video_scores\n",
    "                print_table(video_scores, cell_width=[3,40,8])\n",
    "\n",
    "            # Save model weights\n",
    "            path, filename = os.path.split(self.split_file)\n",
    "            base_filename, _ = os.path.splitext(filename)\n",
    "            path = os.path.join(output_dir, 'models_temp', base_filename+'_'+str(self.split_id))\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            filename = str(epoch)+'_'+str(round(val_fscore*100,3))+'.pth.tar'\n",
    "            torch.save(self.model.state_dict(), os.path.join(path, filename))\n",
    "\n",
    "        return max_val_fscore, max_val_fscore_epoch\n",
    "\n",
    "\n",
    "    def eval(self, dataset, keys=None, results_filename=None):\n",
    "\n",
    "        self.model.eval()\n",
    "        summary = {}\n",
    "        att_vecs = {}\n",
    "        \n",
    "        with torch.no_grad(), h5py.File(self.datasets_dict[dataset], 'a') as d:\n",
    "            if keys==None:\n",
    "                keys=d.keys()\n",
    "                \n",
    "            for i, key in enumerate(keys):\n",
    "                # data = self.get_data(key)\n",
    "                # seq = self.dataset[key]['features'][...]\n",
    "                seq = d[key]['features'][...]\n",
    "                seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "\n",
    "                if self.hps.use_cuda:\n",
    "                    seq = seq.float().cuda()\n",
    "\n",
    "                y, att_vec = self.model(seq, seq.shape[1])\n",
    "                summary[key] = y[0].detach().cpu().numpy()\n",
    "                att_vecs[key] = att_vec.detach().cpu().numpy()\n",
    "\n",
    "            f_score, video_scores = self.eval_summary(summary, keys,  d, results_filename=results_filename, metric=self.hps.dataset_name, att_vecs=att_vecs)\n",
    "\n",
    "        return f_score, video_scores\n",
    "\n",
    "\n",
    "    def eval_summary(self, machine_summary_activations, test_keys, dataset,  results_filename=None, metric='tvsum', att_vecs=None):\n",
    "\n",
    "        eval_metric = 'avg' if metric == 'tvsum' else 'max'\n",
    "\n",
    "        if results_filename is None:\n",
    "            results_filename = 'results/test_result001.h5'\n",
    "        fms = []\n",
    "        video_scores = []\n",
    "\n",
    "        with h5py.File(results_filename, 'w') as h5_res:\n",
    "        \n",
    "            for key_idx, key in enumerate(test_keys):\n",
    "                d = dataset[key]\n",
    "                probs = machine_summary_activations[key]\n",
    "\n",
    "                if 'change_points' not in d:\n",
    "                    print(\"ERROR: No change points in dataset/video \",key)\n",
    "\n",
    "                cps = d['change_points'][...]\n",
    "                num_frames = d['n_frames'][()]\n",
    "                nfps = d['n_frame_per_seg'][...].tolist()\n",
    "                positions = d['picks'][...]\n",
    "                # user_summary = d['user_summary'][...]\n",
    "\n",
    "                machine_summary = generate_summary(probs, cps, num_frames, nfps, positions)\n",
    "                # fm, _, _ = evaluate_summary(machine_summary, user_summary, eval_metric)\n",
    "                # fms.append(fm)\n",
    "\n",
    "                # Reporting & logging\n",
    "                video_scores.append([key_idx + 1, key, \"{:.1%}\".format(fm)])\n",
    "\n",
    "                if results_filename:\n",
    "                    gt = d['gtscore'][...]\n",
    "                    h5_res.create_dataset(key + '/score', data=probs)\n",
    "                    h5_res.create_dataset(key + '/machine_summary', data=machine_summary)\n",
    "                    h5_res.create_dataset(key + '/gtscore', data=gt)\n",
    "                    # h5_res.create_dataset(key + '/fm', data=fm)\n",
    "                    h5_res.create_dataset(key + '/picks', data=positions)\n",
    "\n",
    "                    video_name = key.split('/')[1]\n",
    "                    if 'video_name' in d:\n",
    "                        video_name = d['video_name'][...]\n",
    "                    h5_res.create_dataset(key + '/video_name', data=video_name)\n",
    "\n",
    "                    if att_vecs is not None:\n",
    "                        h5_res.create_dataset(key + '/att', data=att_vecs[key])\n",
    "\n",
    "        mean_fm = np.mean(fms)\n",
    "\n",
    "        # Reporting & logging\n",
    "        # if results_filename is not None:\n",
    "        #     h5_res.close()\n",
    "\n",
    "        return mean_fm, video_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d96f2-e26a-4537-ac9d-cc86dd634562",
   "metadata": {},
   "source": [
    "#### Testing snd Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcf64d6d-ac83-46a9-adaa-68fdebe693d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_videos(hps, args):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    ao = AONet(hps)\n",
    "    ao.initialize()\n",
    "    datasets_dict = ao.load_datasets(args['datasets'])\n",
    "    # ao.load_split_file(splits_filename)\n",
    "\n",
    "    val_fscores = []\n",
    "    \n",
    "    # ao.select_split(split_id)\n",
    "    # weights_filename, _ = ao.lookup_weights_file(data_dir)\n",
    "    # print(\"Loading model:\", weights_filename)\n",
    "    # ao.load_model(weights_filename)\n",
    "    \n",
    "    ao.load_model(args['model'])\n",
    "    for dataset in datasets_dict.keys():\n",
    "        val_fscore, video_scores = ao.eval(dataset, results_filename=args['result']+'/'+dataset+'.h5')\n",
    "        val_fscores.append(val_fscore)\n",
    "\n",
    "    val_fscore_avg = np.mean(val_fscores)\n",
    "\n",
    "    if hps.verbose:\n",
    "        video_scores = [[\"No.\", \"Video\", \"F-score\"]] + video_scores\n",
    "        print_table(video_scores, cell_width=[4,45,5])\n",
    "\n",
    "    print(\"Avg F-score: \", val_fscore)\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Total AVG F-score: \", val_fscore_avg)\n",
    "    return val_fscore_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e712a2-d105-453e-8934-3ed8eccc6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'verbose':True,\n",
    "    'use_cuda':False,\n",
    "    'cuda_device':0,\n",
    "    'max_summary_length':0.15,\n",
    "    # 'dataset':['../../Preprocessing/extracted_features/normal/TVSum.h5','../../Preprocessing/extracted_features/Prebuilt/eccv16_dataset_tvsum_google_pool5.h5'],\n",
    "    \n",
    "    'datasets':'../../Preprocessing/extracted_features/Prebuilt/eccv16_dataset_tvsum_google_pool5.h5, ',\n",
    "    'splits':None,\n",
    "    \"train\" : False,\n",
    "    \"model\" : 'data/models/tvsum_splits_4_0.5941821875878188.tar.pth', \n",
    "    \"result\": 'results'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b752a6-6f19-45f2-ae5b-946b5cc250b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de6d44f-b766-44c8-abcb-103e6d86f462",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### )) dtghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45819dc8-8ba5-49dd-818b-90af7a0478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mainFn(args):\n",
    "    hps = HParameters(args)\n",
    "    hps.load_from_args(args)\n",
    "    \n",
    "    print(\"Parameters:\")\n",
    "    print(hps)\n",
    "    results=[['No', 'Split', 'Mean F-score']]\n",
    "    f_score = evaluate_videos(hps, args)\n",
    "    results.append([i+1, split_filename, str(round(f_score * 100.0, 3))+\"%\"])\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print_table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af46d849-cbe7-4418-aefc-9c4725d2132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainFn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4c897-5df2-45a1-b98c-641afddffb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508692a-ff2f-4e94-b084-9700ba8986c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877cd24-69e8-43a9-abea-9809d71d845f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea0490d-c311-4e5e-891c-55e5bd64c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_split(hps, splits_filename, data_dir='test'):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    ao = AONet(hps)\n",
    "    ao.initialize()\n",
    "    datasets = ao.load_datasets()\n",
    "    # ao.load_split_file(splits_filename)\n",
    "\n",
    "    val_fscores = []\n",
    "    ao.load_model(data_dir)\n",
    "    \n",
    "        \n",
    "    for split_id in range(len(ao.splits)):\n",
    "        ao.select_split(split_id)\n",
    "        # weights_filename, _ = ao.lookup_weights_file(data_dir)\n",
    "        # print(\"Loading model:\", weights_filename)\n",
    "        # ao.load_model(weights_filename)\n",
    "        ao.load_model(data_dir)\n",
    "        val_fscore, video_scores = ao.eval(ao.test_keys, results_filename=args['result'])\n",
    "        val_fscores.append(val_fscore)\n",
    "\n",
    "        val_fscore_avg = np.mean(val_fscores)\n",
    "\n",
    "        if hps.verbose:\n",
    "            video_scores = [[\"No.\", \"Video\", \"F-score\"]] + video_scores\n",
    "            print_table(video_scores, cell_width=[4,45,5])\n",
    "\n",
    "        print(\"Avg F-score: \", val_fscore)\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Total AVG F-score: \", val_fscore_avg)\n",
    "    return val_fscore_avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2223a-713c-417a-ad2a-f543c805068e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AONet:\n",
    "\n",
    "    def __init__(self, hps: HParameters):\n",
    "        self.hps = hps\n",
    "        self.model = None\n",
    "        self.log_file = None\n",
    "        self.verbose = hps.verbose\n",
    "\n",
    "\n",
    "    def initialize(self, cuda_device=None):\n",
    "        rnd_seed = 12345\n",
    "        random.seed(rnd_seed)\n",
    "        np.random.seed(rnd_seed)\n",
    "        torch.manual_seed(rnd_seed)\n",
    "\n",
    "        self.model = VASNet()\n",
    "        self.model.eval()\n",
    "        self.model.apply(weights_init)##?\n",
    "        #print(self.model)\n",
    "\n",
    "        cuda_device = cuda_device or self.hps.cuda_device\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            print(\"Setting CUDA device: \",cuda_device)\n",
    "            torch.cuda.set_device(cuda_device)\n",
    "            torch.cuda.manual_seed(rnd_seed)\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def load_datasets(self, datasets = None):\n",
    "        \"\"\"\n",
    "        Loads all h5 datasets from the datasets list into a dictionary self.dataset\n",
    "        referenced by their base filename\n",
    "        :param datasets:  List of dataset filenames\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if datasets is None:\n",
    "            datasets = self.hps.datasets\n",
    "\n",
    "        datasets_dict = {}\n",
    "        for dataset in datasets:\n",
    "            _, base_filename = os.path.split(dataset)\n",
    "            base_filename, _ = os.path.splitext(base_filename)\n",
    "            print(\"Loading:\", dataset)\n",
    "            # dataset_name = base_filename.split('_')[2]\n",
    "            # print(\"\\tDataset name:\", dataset_name)\n",
    "            # datasets_dict[base_filename] = h5py.File(dataset, 'r')\n",
    "            datasets_dict[base_filename] = dataset\n",
    "\n",
    "        self.datasets_dict = datasets_dict\n",
    "        \n",
    "        return datasets_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_model(self, model_filename):\n",
    "        self.model.load_state_dict(torch.load(model_filename, map_location=lambda storage, loc: storage))\n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def fix_keys(self, keys, dataset_name = None):\n",
    "#         \"\"\"\n",
    "#         :param keys:\n",
    "#         :return:\n",
    "#         \"\"\"\n",
    "#         # dataset_name = None\n",
    "#         if len(self.datasets) == 1:\n",
    "#             dataset_name = next(iter(self.datasets))\n",
    "\n",
    "#         keys_out = []\n",
    "#         for key in keys:\n",
    "#             t = key.split('/')\n",
    "#             if len(t) != 2:\n",
    "#                 assert dataset_name is not None, \"ERROR dataset name in some keys is missing but there are multiple dataset {} to choose from\".format(len(self.datasets))\n",
    "\n",
    "#                 key_name = dataset_name+'/'+key\n",
    "#                 keys_out.append(key_name)\n",
    "#             else:\n",
    "#                 keys_out.append(key)\n",
    "\n",
    "#         return keys_out\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def load_split_file(self, splits_file):\n",
    "\n",
    "        self.dataset_name, self.dataset_type, self.splits = parse_splits_filename(splits_file)\n",
    "        n_folds = len(self.splits)\n",
    "        self.split_file = splits_file\n",
    "        print(\"Loading splits from: \",splits_file)\n",
    "\n",
    "        return n_folds\n",
    "\n",
    "\n",
    "#     def select_split(self, split_id):\n",
    "#         print(\"Selecting split: \",split_id)\n",
    "\n",
    "#         self.split_id = split_id\n",
    "#         n_folds = len(self.splits)\n",
    "#         assert self.split_id < n_folds, \"split_id (got {}) exceeds {}\".format(self.split_id, n_folds)\n",
    "\n",
    "#         split = self.splits[self.split_id]\n",
    "#         self.train_keys = split['train_keys']\n",
    "#         self.test_keys = split['test_keys']\n",
    "\n",
    "#         dataset_filename = self.hps.get_dataset_by_name(self.dataset_name)[0]\n",
    "#         _,dataset_filename = os.path.split(dataset_filename)\n",
    "#         dataset_filename,_ = os.path.splitext(dataset_filename)\n",
    "#         self.train_keys = self.fix_keys(self.train_keys, dataset_filename)\n",
    "#         self.test_keys = self.fix_keys(self.test_keys, dataset_filename)\n",
    "#         return\n",
    "\n",
    "\n",
    "\n",
    "#     def get_data(self, key):\n",
    "#         key_parts = key.split('/')\n",
    "#         assert len(key_parts) == 2, \"ERROR. Wrong key name: \"+key\n",
    "#         dataset, key = key_parts\n",
    "#         return self.datasets[dataset][key]\n",
    "\n",
    "#     def lookup_weights_file(self, data_path):\n",
    "#         dataset_type_str = '' if self.dataset_type == '' else self.dataset_type + '_'\n",
    "#         weights_filename = data_path + '/models/{}_{}splits_{}_*.tar.pth'.format(self.dataset_name, dataset_type_str, self.split_id)\n",
    "#         weights_filename = glob.glob(weights_filename)\n",
    "#         if len(weights_filename) == 0:\n",
    "#             print(\"Couldn't find model weights: \", weights_filename)\n",
    "#             return ''\n",
    "\n",
    "#         # Get the first weights filename in the dir\n",
    "#         weights_filename = weights_filename[0]\n",
    "#         splits_file = data_path + '/splits/{}_{}splits.json'.format(self.dataset_name, dataset_type_str)\n",
    "\n",
    "#         return weights_filename, splits_file\n",
    "\n",
    "\n",
    "    def train(self, output_dir='EX-0'):\n",
    "\n",
    "        print(\"Initializing VASNet model and optimizer...\")\n",
    "        self.model.train()\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=self.hps.lr[0], weight_decay=self.hps.l2_req)\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        max_val_fscore = 0\n",
    "        max_val_fscore_epoch = 0\n",
    "        train_keys = self.train_keys[:]\n",
    "\n",
    "        lr = self.hps.lr[0]\n",
    "        for epoch in range(self.hps.epochs_max):\n",
    "\n",
    "            print(\"Epoch: {0:6}\".format(str(epoch)+\"/\"+str(self.hps.epochs_max)), end='')\n",
    "            self.model.train()\n",
    "            avg_loss = []\n",
    "\n",
    "            random.shuffle(train_keys)\n",
    "\n",
    "            for i, key in enumerate(train_keys):\n",
    "                dataset = self.get_data(key)\n",
    "                seq = dataset['features'][...]\n",
    "                seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "                target = dataset['gt_score'][...]\n",
    "                target = torch.from_numpy(target).unsqueeze(0)\n",
    "\n",
    "                # Normalize frame scores\n",
    "                target -= target.min()\n",
    "                target /= target.max()\n",
    "\n",
    "                if self.hps.use_cuda:\n",
    "                    seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "                seq_len = seq.shape[1]\n",
    "                y, _ = self.model(seq,seq_len)\n",
    "                loss_att = 0\n",
    "\n",
    "                loss = criterion(y, target)\n",
    "                # loss2 = y.sum()/seq_len\n",
    "                loss = loss + loss_att\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_loss.append([float(loss), float(loss_att)])\n",
    "\n",
    "            # Evaluate test dataset\n",
    "            val_fscore, video_scores = self.eval(self.test_keys)\n",
    "            if max_val_fscore < val_fscore:\n",
    "                max_val_fscore = val_fscore\n",
    "                max_val_fscore_epoch = epoch\n",
    "\n",
    "            avg_loss = np.array(avg_loss)\n",
    "            print(\"   Train loss: {0:.05f}\".format(np.mean(avg_loss[:, 0])), end='')\n",
    "            print('   Test F-score avg/max: {0:0.5}/{1:0.5}'.format(val_fscore, max_val_fscore))\n",
    "\n",
    "            if self.verbose:\n",
    "                video_scores = [[\"No\", \"Video\", \"F-score\"]] + video_scores\n",
    "                print_table(video_scores, cell_width=[3,40,8])\n",
    "\n",
    "            # Save model weights\n",
    "            path, filename = os.path.split(self.split_file)\n",
    "            base_filename, _ = os.path.splitext(filename)\n",
    "            path = os.path.join(output_dir, 'models_temp', base_filename+'_'+str(self.split_id))\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            filename = str(epoch)+'_'+str(round(val_fscore*100,3))+'.pth.tar'\n",
    "            torch.save(self.model.state_dict(), os.path.join(path, filename))\n",
    "\n",
    "        return max_val_fscore, max_val_fscore_epoch\n",
    "\n",
    "\n",
    "#     def eval(self, dataset, keys=None, results_filename=None):\n",
    "\n",
    "#         self.model.eval()\n",
    "#         summary = {}\n",
    "#         att_vecs = {}\n",
    "        \n",
    "#         with torch.no_grad(), h5py.File(self.datasets_dict[dataset], 'a') as d:\n",
    "#             if keys==None:\n",
    "#                 keys=d.keys()\n",
    "                \n",
    "#             for i, key in enumerate(keys):\n",
    "#                 # data = self.get_data(key)\n",
    "#                 # seq = self.dataset[key]['features'][...]\n",
    "#                 seq = d[key]['features'][...]\n",
    "#                 seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "\n",
    "#                 if self.hps.use_cuda:\n",
    "#                     seq = seq.float().cuda()\n",
    "\n",
    "#                 y, att_vec = self.model(seq, seq.shape[1])\n",
    "#                 summary[key] = y[0].detach().cpu().numpy()\n",
    "#                 att_vecs[key] = att_vec.detach().cpu().numpy()\n",
    "\n",
    "#             f_score, video_scores = self.eval_summary(summary, keys,  d, results_filename=results_filename, metric=self.hps.dataset_name, att_vecs=att_vecs)\n",
    "\n",
    "#         return f_score, video_scores\n",
    "\n",
    "\n",
    "#     def eval_summary(self, machine_summary_activations, test_keys, dataset,  results_filename=None, metric='tvsum', att_vecs=None):\n",
    "\n",
    "#         eval_metric = 'avg' if metric == 'tvsum' else 'max'\n",
    "\n",
    "#         if results_filename is None:\n",
    "#             results_filename = 'results/test_result001.h5'\n",
    "#         fms = []\n",
    "#         video_scores = []\n",
    "\n",
    "#         with h5py.File(results_filename, 'w') as h5_res:\n",
    "        \n",
    "#             for key_idx, key in enumerate(test_keys):\n",
    "#                 d = dataset[key]\n",
    "#                 probs = machine_summary_activations[key]\n",
    "\n",
    "#                 if 'change_points' not in d:\n",
    "#                     print(\"ERROR: No change points in dataset/video \",key)\n",
    "\n",
    "#                 cps = d['change_points'][...]\n",
    "#                 num_frames = d['n_frames'][()]\n",
    "#                 nfps = d['n_frame_per_seg'][...].tolist()\n",
    "#                 positions = d['picks'][...]\n",
    "#                 # user_summary = d['user_summary'][...]\n",
    "\n",
    "#                 machine_summary = generate_summary(probs, cps, num_frames, nfps, positions)\n",
    "#                 # fm, _, _ = evaluate_summary(machine_summary, user_summary, eval_metric)\n",
    "#                 # fms.append(fm)\n",
    "\n",
    "#                 # Reporting & logging\n",
    "#                 video_scores.append([key_idx + 1, key, \"{:.1%}\".format(fm)])\n",
    "\n",
    "#                 if results_filename:\n",
    "#                     gt = d['gtscore'][...]\n",
    "#                     h5_res.create_dataset(key + '/score', data=probs)\n",
    "#                     h5_res.create_dataset(key + '/machine_summary', data=machine_summary)\n",
    "#                     h5_res.create_dataset(key + '/gtscore', data=gt)\n",
    "#                     # h5_res.create_dataset(key + '/fm', data=fm)\n",
    "#                     h5_res.create_dataset(key + '/picks', data=positions)\n",
    "\n",
    "#                     video_name = key.split('/')[1]\n",
    "#                     if 'video_name' in d:\n",
    "#                         video_name = d['video_name'][...]\n",
    "#                     h5_res.create_dataset(key + '/video_name', data=video_name)\n",
    "\n",
    "#                     if att_vecs is not None:\n",
    "#                         h5_res.create_dataset(key + '/att', data=att_vecs[key])\n",
    "\n",
    "#         mean_fm = np.mean(fms)\n",
    "\n",
    "#         # Reporting & logging\n",
    "#         # if results_filename is not None:\n",
    "#         #     h5_res.close()\n",
    "\n",
    "#         return mean_fm, video_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09359075-0c6a-4b86-aeda-ecc687e5c2b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(hps):\n",
    "    # os.makedirs(hps.output_dir, exist_ok=True)\n",
    "    # os.makedirs(os.path.join(hps.output_dir, 'splits'), exist_ok=True)\n",
    "    # os.makedirs(os.path.join(hps.output_dir, 'code'), exist_ok=True)\n",
    "    # os.makedirs(os.path.join(hps.output_dir, 'models'), exist_ok=True)\n",
    "    # os.system('cp -f splits/*.json  ' + hps.output_dir + '/splits/')\n",
    "    # os.system('cp *.py ' + hps.output_dir + '/code/')\n",
    "\n",
    "    # Create a file to collect results from all splits\n",
    "    f = open(hps.results_path, 'wt')\n",
    "           \n",
    "        \n",
    "    # for split_filename in hps.split_file:\n",
    "    #     dataset_name, dataset_type, splits = parse_splits_filename(split_filename)\n",
    "\n",
    "        # For no augmentation use only a dataset corresponding to the split file\n",
    "#         datasets = None\n",
    "#         if dataset_type == '':\n",
    "#             datasets = hps.get_dataset_by_name(dataset_name)\n",
    "\n",
    "#         if datasets is None:\n",
    "#             datasets = hps.datasets\n",
    "\n",
    "    f_avg = 0\n",
    "    f = open(hps.split_file)\n",
    "    splits = json.load(f)\n",
    "    n_folds = len(splits)\n",
    "    for split_id in range(n_folds):\n",
    "        ao = AONet(hps)\n",
    "        ao.initialize()\n",
    "        ao.load_datasets(datasets=datasets)\n",
    "        ao.load_split_file(splits_file=split_filename)\n",
    "        ao.select_split(split_id=split_id)\n",
    "\n",
    "        fscore, fscore_epoch = ao.train(output_dir=hps.output_dir)\n",
    "        f_avg += fscore\n",
    "\n",
    "        # Log F-score for this split_id\n",
    "        f.write(split_filename + ', ' + str(split_id) + ', ' + str(fscore) + ', ' + str(fscore_epoch) + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        # Save model with the highest F score\n",
    "        _, log_file = os.path.split(split_filename)\n",
    "        log_dir, _ = os.path.splitext(log_file)\n",
    "        log_dir += '_' + str(split_id)\n",
    "        log_file = os.path.join(hps.output_dir, 'models', log_dir) + '_' + str(fscore) + '.tar.pth'\n",
    "\n",
    "        os.makedirs(os.path.join(hps.output_dir, 'models', ), exist_ok=True)\n",
    "        os.system('mv ' + hps.output_dir + '/models_temp/' + log_dir + '/' + str(fscore_epoch) + '_*.pth.tar ' + log_file)\n",
    "        os.system('rm -rf ' + hps.output_dir + '/models_temp/' + log_dir)\n",
    "\n",
    "        print(\"Split: {0:}   Best F-score: {1:0.5f}   Model: {2:}\".format(split_filename, fscore, log_file))\n",
    "\n",
    "    # Write average F-score for all splits to the results.txt file\n",
    "    f_avg /= n_folds\n",
    "    f.write(split_filename + ', ' + str('avg') + ', ' + str(f_avg) + '\\n')\n",
    "    f.flush()\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ee811-c01a-4c60-8478-2d1c8a997895",
   "metadata": {},
   "source": [
    "## Vasnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbee983-1aa9-4cd4-9ea5-9c15170c7177",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30e69143-7582-4c9e-81de-c9fe7b44c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "import import_ipynb\n",
    "from Model import VASNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e3eb6-8ea5-4d5e-b8d1-47b4b00d9e55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Util modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932d395f-b166-4925-8297-7c44f438fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "def write_json(obj, fpath):\n",
    "    mkdir_if_missing(osp.dirname(fpath))\n",
    "    with open(fpath, 'w') as f:\n",
    "        json.dump(obj, f, indent=4, separators=(',', ': '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61b34d5f-84d5-4c92-ac7c-c0c22a76a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_random(keys, num_videos, num_train):\n",
    "    \"\"\"Random split\"\"\"\n",
    "    train_keys, test_keys = [], []\n",
    "    rnd_idxs = np.random.choice(range(num_videos), size=num_train, replace=False)\n",
    "    for key_idx, key in enumerate(keys):\n",
    "        if key_idx in rnd_idxs:\n",
    "            train_keys.append(key)\n",
    "        else:\n",
    "            test_keys.append(key)\n",
    "\n",
    "    assert len(set(train_keys) & set(test_keys)) == 0, \"Error: train_keys and test_keys overlap\"\n",
    "\n",
    "    return train_keys, test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d203a6e8-05a9-426b-9d80-220141b9d33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_splits_filename(splits_filename):\n",
    "    # Parse split file and count number of k_folds\n",
    "    spath, sfname = os.path.split(splits_filename)\n",
    "    sfname, _ = os.path.splitext(sfname)\n",
    "    dataset_name = sfname.split('_')[0]  # Get dataset name e.g. tvsum\n",
    "    dataset_type = sfname.split('_')[1]  # augmentation type e.g. aug\n",
    "\n",
    "    # The keyword 'splits' is used as the filename fields terminator from historical reasons.\n",
    "    if dataset_type == 'splits':\n",
    "        # Split type is not present\n",
    "        dataset_type = ''\n",
    "\n",
    "    # Get number of discrete splits within each split json file\n",
    "    with open(splits_filename, 'r') as sf:\n",
    "        splits = json.load(sf)\n",
    "\n",
    "    return dataset_name, dataset_type, splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8086414-51b2-4d85-9061-b55a3a2488a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname == 'Linear':\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf147d9-45af-4a83-ae5e-79fef04e592a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "298da162-c1d0-4231-af02-4520cc5ad1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HParameters:\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        \n",
    "        self.verbose = args['verbose']\n",
    "        self.use_cuda = args['use_cuda']\n",
    "        self.cuda_device = args['cuda_device']\n",
    "        self.max_summary_length = args['max_summary_length']\n",
    "\n",
    "        self.l2_req = 0.00001\n",
    "        self.lr_epochs = [0]\n",
    "        self.lr = [0.00005]\n",
    "        self.epochs_max = 300\n",
    "        self.train_batch_size = 1\n",
    "\n",
    "        self.dataset=args['dataset']\n",
    "        self.results_path = args['results_path']\n",
    "        self.num_splits = args['num_splits']\n",
    "        self.split_file = args['split_file']\n",
    "        self.train_percent = args['train_percent']\n",
    "        \n",
    "        if 'model_path' in args:\n",
    "            self.model_path = args['model_path']\n",
    "        else:\n",
    "            self.model_path = None\n",
    "        return\n",
    "\n",
    "\n",
    "    def create_split(self):\n",
    "        print(\"Loading dataset from {}\".format(self.dataset))\n",
    "        \n",
    "        with h5py.File(self.dataset, 'r') as dataset:\n",
    "            keys = dataset.keys()\n",
    "            num_videos = len(keys)\n",
    "            num_train = int(math.ceil(num_videos * self.train_percent))\n",
    "            num_test = num_videos - num_train\n",
    "\n",
    "            print(\"Split breakdown: # total videos {}. # train videos {}. # test videos {}\".format(num_videos, num_train, num_test))\n",
    "            splits = []\n",
    "\n",
    "            for split_idx in range(self.num_splits):\n",
    "                train_keys, test_keys = split_random(keys, num_videos, num_train)\n",
    "                splits.append({\n",
    "                    'train_keys': train_keys,\n",
    "                    'test_keys': test_keys,\n",
    "                    })\n",
    "\n",
    "            # saveto = osp.join(self.split_file)\n",
    "            write_json(splits, self.split_file)\n",
    "            print(\"Splits saved to {}\".format(self.split_file))\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        vars = [attr for attr in dir(self) if not callable(getattr(self,attr)) and not (attr.startswith(\"__\") or attr.startswith(\"_\"))]\n",
    "\n",
    "        info_str = ''\n",
    "        for i, var in enumerate(vars):\n",
    "            val = getattr(self, var)\n",
    "            if isinstance(val, Variable):\n",
    "                val = val.data.cpu().numpy().tolist()[0]\n",
    "            info_str += '['+str(i)+'] '+var+': '+str(val)+'\\n'\n",
    "\n",
    "        return info_str\n",
    "    \n",
    "    \n",
    "#     def load_from_args(self, args):\n",
    "#         for key in args:\n",
    "#             val = args[key]\n",
    "#             if val is not None:\n",
    "#                 if hasattr(self, key) and isinstance(getattr(self, key), list):\n",
    "#                     val = val.split()\n",
    "\n",
    "#                 setattr(self, key, val)\n",
    "\n",
    "#     def get_dataset_by_name(self, dataset_name):\n",
    "#         for d in self.datasets:\n",
    "#             if dataset_name in d:\n",
    "#                 return [d]\n",
    "#         return None\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ec2db-e6eb-408f-9442-1e861711badc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "301dbd28-1415-4409-bb4d-821d0a41f72c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, hps: HParameters):\n",
    "        self.hps = hps\n",
    "        self.model = VASNet()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.show_every = 1\n",
    "        \n",
    "\n",
    "        \n",
    "    def init_model(self):\n",
    "        if self.hps.model_path:\n",
    "            self.model.load_state_dict(torch.load(self.hps.model_path, map_location=lambda storage, loc: storage))\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            self.model.apply(weights_init)\n",
    "        \n",
    "        \n",
    "    def train(self, train_keys):\n",
    "        losses=[]\n",
    "        for i, key in enumerate(train_keys):\n",
    "            with h5py.File(self.hps.dataset) as d:\n",
    "                seq= d[key]['features'][...]\n",
    "                target = d[key]['gt_score'][...]\n",
    "                target = target.astype(float)\n",
    "                \n",
    "            # seq = dataset['features'][...]\n",
    "            seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "            # target = dataset['gtscore'][...]\n",
    "            target = torch.from_numpy(target).unsqueeze(0)\n",
    "\n",
    "            # Min-Max Normalize frame scores\n",
    "            target -= target.min()\n",
    "            target /= target.max()\n",
    "            \n",
    "\n",
    "            if self.hps.use_cuda:\n",
    "                seq, target = seq.float().cuda(), target.float().cuda()\n",
    "\n",
    "            seq_len = seq.shape[1]\n",
    "            y, _ = self.model(seq,seq_len)\n",
    "            # print(y)\n",
    "            loss_att = 0\n",
    "\n",
    "            loss = self.criterion(y, target.float())\n",
    "            loss = loss + loss_att\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "            \n",
    "        return np.mean(np.array(losses))\n",
    "\n",
    "#     def video_fsore():\n",
    "#          eval_metric = 'avg' if metric == 'tvsum' else 'max'\n",
    "\n",
    "#         if results_filename is not None:\n",
    "#             h5_res = h5py.File(results_filename, 'w')\n",
    "\n",
    "#         fms = []\n",
    "#         video_scores = []\n",
    "#         for key_idx, key in enumerate(test_keys):\n",
    "#             d = self.get_data(key)\n",
    "#             probs = machine_summary_activations[key]\n",
    "\n",
    "#             if 'change_points' not in d:\n",
    "#                 print(\"ERROR: No change points in dataset/video \",key)\n",
    "\n",
    "#             cps = d['change_points'][...]\n",
    "#             num_frames = d['n_frames'][()]\n",
    "#             nfps = d['n_frame_per_seg'][...].tolist()\n",
    "#             positions = d['picks'][...]\n",
    "#             user_summary = d['user_summary'][...]\n",
    "\n",
    "#             machine_summary = generate_summary(probs, cps, num_frames, nfps, positions)\n",
    "#             fm, _, _ = evaluate_summary(machine_summary, user_summary, eval_metric)\n",
    "#             fms.append(fm)\n",
    "\n",
    "#             # Reporting & logging\n",
    "#             video_scores.append([key_idx + 1, key, \"{:.1%}\".format(fm)])\n",
    "\n",
    "#     def validate(self, test_keys):\n",
    "#         self.model.eval()\n",
    "#         summary = {}\n",
    "#         att_vecs = {}\n",
    "#         with torch.no_grad():\n",
    "#             for i, key in enumerate(test_keys):\n",
    "#                 with h5py.File() as d:\n",
    "#                     data = d[key][...]\n",
    "#                 seq = data['features'][...]\n",
    "#                 seq = torch.from_numpy(seq).unsqueeze(0)\n",
    "\n",
    "#                 if self.hps.use_cuda:\n",
    "#                     seq = seq.float().cuda()\n",
    "\n",
    "#                 y, att_vec = self.model(seq, seq.shape[1])\n",
    "#                 summary[key] = y[0].detach().cpu().numpy()\n",
    "#                 att_vecs[key] = att_vec.detach().cpu().numpy()\n",
    "\n",
    "#         f_score, video_scores = self.eval_summary(summary, keys, metric=self.dataset_name,\n",
    "#                     results_filename=results_filename, att_vecs=att_vecs)\n",
    "        \n",
    "    def run(self):\n",
    "        print(\"Initializing VASNet model and optimizer...\")\n",
    "        self.init_model()\n",
    "        self.model.train()\n",
    "\n",
    "        if self.hps.use_cuda:\n",
    "            self.criterion = self.criterion.cuda()\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=self.hps.lr[0], weight_decay=self.hps.l2_req)\n",
    "        \n",
    "        lr = self.hps.lr[0]\n",
    "        \n",
    "        f = open(hps.split_file)\n",
    "        splits = json.load(f)\n",
    "        n_folds = len(splits)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        for split in splits:\n",
    "            max_val_fscore = 0\n",
    "            max_val_fscore_epoch = 0\n",
    "            train_keys = split['train_keys']\n",
    "            test_keys = split['test_keys']\n",
    "\n",
    "            epoch_losses=[]\n",
    "            for epoch in range(self.hps.epochs_max):\n",
    "\n",
    "                print(\"Epoch: {0:6}\".format(str(epoch)+\"/\"+str(self.hps.epochs_max)), end='')\n",
    "                self.model.train()\n",
    "\n",
    "                random.shuffle(train_keys)   \n",
    "                loss = self.train(train_keys)\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "                \n",
    "                # Evaluate test dataset\n",
    "                # val_fscore, video_scores = self.validate(self.test_keys)\n",
    "                # if max_val_fscore < val_fscore:\n",
    "                #     max_val_fscore = val_fscore\n",
    "                #     max_val_fscore_epoch = epoch\n",
    "                \n",
    "                if epoch%self.show_every==0:\n",
    "                    print(f'Epoch:{epoch}, Loss:{loss}')\n",
    "\n",
    "            # avg_loss = np.array(epoch_losses)\n",
    "            print(\"   Train loss: {0:.05f}\".format(np.mean(np.array(epoch_losses))), end='')\n",
    "            # print('   Test F-score avg/max: {0:0.5}/{1:0.5}'.format(val_fscore, max_val_fscore))\n",
    "\n",
    "            # if self.verbose:\n",
    "            #     video_scores = [[\"No\", \"Video\", \"F-score\"]] + video_scores\n",
    "            #     print_table(video_scores, cell_width=[3,40,8])\n",
    "\n",
    "        # return max_val_fscore, , max_val_fscore_epoch\n",
    "    \n",
    "    def save_model(self,splitn):\n",
    "        # Save model weights\n",
    "        path, filename = os.path.split(self.split_file)\n",
    "        base_filename, _ = os.path.splitext(filename)\n",
    "        # path = os.path.join(output_dir, 'models_temp', base_filename+'_'+str(self.split_id))\n",
    "        # os.makedirs(path, exist_ok=True)\n",
    "        # filename = base_filename+'_'+str(epoch)+'_'+str(round(val_fscore*100,3))+'.pth.tar'\n",
    "        filename = base_filename+'_'+str(epoch)+'_'+splitn+'.pth.tar'\n",
    "        torch.save(self.model.state_dict(), os.path.join('models', filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933537a9-9009-4d10-b65b-3d5b8a5f2af1",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ee8605c-fa7c-4196-9abd-aadf592dfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\n",
    "    'results_path':'training_results.txt',\n",
    "    'num_splits':5,\n",
    "    'split_file':'splits/test_split1.json',\n",
    "    'dataset': '../../Preprocessing/extracted_features/normal/TVSum.h5',\n",
    "    'train_percent':0.8,\n",
    "    'verbose':True,\n",
    "    'use_cuda' : False,\n",
    "    'cuda_device': None,\n",
    "    'max_summary_length': 0.15\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ae8df64-01ae-4755-a6e2-39f814bb4035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from ../../Preprocessing/extracted_features/normal/TVSum.h5\n",
      "Split breakdown: # total videos 50. # train videos 40. # test videos 10\n",
      "Splits saved to splits/test_split1.json\n"
     ]
    }
   ],
   "source": [
    "hps = HParameters(args)\n",
    "# hps.load_from_args(args.__dict__)\n",
    "hps.create_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a952eddd-12f4-49dc-b378-ee145803170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../../Preprocessing/extracted_features/normal/TVSum.h5') as d:                \n",
    "    target = d[key]['gt_score'][...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0cc69-1791-4d60-8568-2d8c9711763b",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
